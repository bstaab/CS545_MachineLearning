{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\xv}{\\mathbf{x}}\n",
    "\\newcommand{\\Xv}{\\mathbf{X}}\n",
    "\\newcommand{\\Xvh}{\\hat{\\mathbf{X}}}\n",
    "\\newcommand{\\yv}{\\mathbf{y}}\n",
    "\\newcommand{\\Yv}{\\mathbf{Y}}\n",
    "\\newcommand{\\av}{\\mathbf{a}}\n",
    "\\newcommand{\\Wv}{\\mathbf{W}}\n",
    "\\newcommand{\\wv}{\\mathbf{w}}\n",
    "\\newcommand{\\gv}{\\mathbf{g}}\n",
    "\\newcommand{\\Hv}{\\mathbf{H}}\n",
    "\\newcommand{\\dv}{\\mathbf{d}}\n",
    "\\newcommand{\\Vv}{\\mathbf{V}}\n",
    "\\newcommand{\\vv}{\\mathbf{v}}\n",
    "\\newcommand{\\Uv}{\\mathbf{U}}\n",
    "\\newcommand{\\tv}{\\mathbf{t}}\n",
    "\\newcommand{\\Tv}{\\mathbf{T}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\Zv}{\\mathbf{Z}}\n",
    "\\newcommand{\\Zvh}{\\hat{\\mathbf{Z}}}\n",
    "\\newcommand{\\Ev}{\\mathbf{E}}\n",
    "\\newcommand{\\onev}{\\mathbf{1}}\n",
    "\\newcommand{\\muv}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\sigmav}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\phiv}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\Phiv}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\Sigmav}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\Lambdav}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\newcommand{\\argmin}[1]{\\underset{#1}{\\operatorname{argmin}}}\n",
    "\\newcommand{\\dimensionbar}[1]{\\underset{#1}{\\operatorname{|}}}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2.1 Three-Layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment, we modify the two-layer neural network matrix equations and code from lecture notes to become a three-layer neural network, one with two hidden layers.  In the following diagram, you will be adding another layer on the left between in the inputs, $x$, and the show hiddden layer.\n",
    "\n",
    "You must complete three steps.\n",
    "1. Complete the matrix equations using latex notation in the markdown cell,\n",
    "2. Implement the required functions in python.\n",
    "3. Perform the described experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Diagram\n",
    "Three-layer neural network with two hidden layers\n",
    "\n",
    "![Two Layers](twoHiddenLayers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\begin{align*}\n",
    "N &= \\text{ number of samples } \\\\\n",
    "I &= \\text{ the number of attributes in each sample }\\\\\n",
    "K &= \\text{ number of units in output layer } \\\\\n",
    "H_1 &= \\text{ number of units in first hidden layer }\\\\\n",
    "H_2 &= \\text{ number of units in second hidden layer }\\\\\n",
    "~\\\\\n",
    "\\Zv_1 &= \\tanh(\\Xvh\\, \\Uv) \\\\\n",
    "\\Zv_2 &= \\tanh(\\Zvh_1\\, \\Vv) \\\\\n",
    "\\Yv &= \\Zvh_2\\, \\Wv\\\\\n",
    "\\Ev &= \\frac{1}{NK} \\sum_{n=1}^N \\sum_{k=1}^K (\\Tv_{n,k} - \\Yv_{n,k})^2 \\\\\n",
    "~\\\\\n",
    "%------------------------------------\n",
    "% Gradient of 'E' with respect to 'Y'\n",
    "\\nabla_\\Yv E_{n,k} &= \\frac{-2}{NK} (\\Tv_{n,k} - \\Yv_{n,k})\\\\\n",
    "~\\\\ \n",
    "%------------------------------------\n",
    "% Gradient of 'E' with respect to 'W'\n",
    "\\nabla_\\Wv E &=  \\underbrace{\\underbrace{\\Zvh_2^T}_{H+1\\times N} \\underbrace{\\delta_\\Yv}_{N\\times K}}_{H+1\\times K} \\;\\;\\;\\;\\text{ where } \\delta_\\Yv = \\frac{-2}{NK} (\\Tv - \\Yv)\\\\\n",
    "~\\\\ \n",
    "%------------------------------------\n",
    "% Gradient of 'E' with respect to 'V'\n",
    "\\nabla_\\Vv E &= \\underbrace{\\underbrace{\\Zvh_1^T}_{I+1\\times N} \\; \\underbrace{\\delta_{\\Zv_2}}_{N\\times H}}_{I+1\\times H} \\;\\;\\;\\;\\text{ where } \\delta_{\\Zv_2} = (\\delta_\\Yv \\; \\Wv_{1:}^T)\\; \\cdot\\; (1-\\Zv_2^2) \\;\\;\\;\\; \\text{ if } f(\\Zvh_1 \\Vv) = \\tanh(\\Xvh \\Vv)\\\\\n",
    "~\\\\ \n",
    "%------------------------------------\n",
    "% Gradient of 'E' with respect to 'U'\n",
    "\\nabla_\\Uv E &= \\underbrace{\\underbrace{\\Xvh^T}_{I+1\\times N} \\; \\underbrace{\\delta_{\\Zv_1}}_{N\\times H}}_{I+1\\times H} \\;\\;\\;\\;\\text{ where } \\delta_{\\Zv_2} = (\\delta_\\Yv \\; \\Wv_{1:}^T)\\; \\cdot\\; (1-\\Zv_1^2) \\;\\;\\;\\; \\text{ if } f(\\Xvh \\Vv) = \\tanh(\\Xvh \\Vv)\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "\n",
    "Modify the functions `network` `error_gradient` and `mse` to do the compuations for a three-layer neural network.  They must have the following arguments:\n",
    "* `Y = network(w, n_inputs, n_hiddens_1, n_hiddens_2, n_outputs, X, all_outputs=False)`\n",
    "or\n",
    "* `Y, Z1, Z2 = network(w, n_inputs, n_hiddens_1, n_hiddens_2, n_outputs, X, all_outputs=True)`\n",
    "* `gradient = error_gradient(w, n_inputs, n_hiddens_1, n_hiddens_2, n_outputs, X, T)`\n",
    "* `mean_squared_error = mse(w, n_inputs, n_hiddens_1, n_hiddens_2, n_outputs, X, T)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Lecture 06 - Make code cell if needed\n",
    "def network__06(w, n_inputs, n_hiddens, n_outputs, X, all_outputs=False):\n",
    "    n_V = (n_inputs + 1) * n_hiddens\n",
    "    n_W = (n_hiddens + 1) * n_outputs\n",
    "    V = w[:n_V].reshape((n_inputs + 1, n_hiddens))\n",
    "    W = w[n_V:].reshape((n_hiddens + 1, n_outputs))\n",
    "    Z = np.tanh(V[0:1, :] + X @ V[1:, :])\n",
    "    Y = W[0:1, :] + Z @ W[1:, :]\n",
    "    return (Y, Z) if all_outputs else Y\n",
    "\n",
    "def error_gradient__06(w, n_inputs, n_hiddens, n_outputs, X, T):\n",
    "    Y, Z = network(w, n_inputs, n_hiddens, n_outputs, X, all_outputs=True)\n",
    "    n_samples = X.shape[0]\n",
    "    delta_Y = -2 / n_samples * (T - Y)\n",
    "    Z_hat = np.insert(Z, 0, 1, axis=1)\n",
    "    dEdW = Z_hat.T @ delta_Y\n",
    "    \n",
    "    n_W = (n_hiddens + 1) * n_outputs\n",
    "    W = w[-n_W:].reshape((n_hiddens + 1, n_outputs))\n",
    "    delta_Z = (delta_Y @ W[1:, :].T) * (1 - Z**2)\n",
    "    X_hat = np.insert(X, 0, 1, axis=1)\n",
    "    dEdV = X_hat.T @ delta_Z\n",
    " \n",
    "    dEdw = np.hstack((dEdV.flatten(), dEdW.flatten()))\n",
    "\n",
    "    return dEdw\n",
    "\n",
    "def mse__06(w, n_inputs, n_hiddens, n_outputs, X, T):\n",
    "    Y = network(w, n_inputs, n_hiddens, n_outputs, X)\n",
    "    return np.mean((T - Y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def network__06(w, n_inputs, n_hiddens, n_outputs, X, all_outputs=False):\n",
    "#    n_V = (n_inputs + 1) * n_hiddens\n",
    "#    n_W = (n_hiddens + 1) * n_outputs\n",
    "#    V = w[:n_V].reshape((n_inputs + 1, n_hiddens))\n",
    "#    W = w[n_V:].reshape((n_hiddens + 1, n_outputs))\n",
    "#    Z = np.tanh(V[0:1, :] + X @ V[1:, :])\n",
    "#    Y = W[0:1, :] + Z @ W[1:, :]\n",
    "#    return (Y, Z) if all_outputs else Y\n",
    "def network(w, n_inputs, n_hiddens_1, n_hiddens_2, n_outputs, X, all_outputs=False):\n",
    "    n_U = (n_inputs + 1) * n_hiddens_1\n",
    "    n_V = (n_hiddens_1 + 1) * n_hiddens_2\n",
    "    n_W = (n_hiddens_2 + 1) * n_outputs\n",
    "    U = w[0:n_U].reshape((n_inputs + 1, n_hiddens_1))\n",
    "    V = w[n_U: n_U + n_V].reshape((n_hiddens_1 + 1, n_hiddens_2))\n",
    "    W = w[n_U + n_V:].reshape((n_hiddens_2 + 1, n_outputs))\n",
    "\n",
    "#   Z   = np.tanh(V[0:1, :] + X @ V[1:, :])    \n",
    "    Z_1 = np.tanh(U[0:1, :] + X @ U[1:, :])\n",
    "    Z_2 = np.tanh(V[0:1, :] + Z_1 @ V[1:, :])\n",
    "    Y = W[0:1, :] + Z_2 @ W[1:, :]\n",
    "    return (Y, Z) if all_outputs else Y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification from A2grader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas\n",
    "import optimizers as opt  # from Lecture Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(3 * 4).reshape((3, 4)) * 0.1\n",
    "T = np.hstack(( np.sin(X[:, 0:1]) + X[:, 1:2],\n",
    "                X[:, 2:3] * -0.5,\n",
    "                X[:, 3:4] ** 2))\n",
    "n_inputs = X.shape[1]\n",
    "n_outputs = T.shape[1]\n",
    "n_hiddens_1 = 6\n",
    "n_hiddens_2 = 2\n",
    "n_w = (n_inputs + 1) * n_hiddens_1 + (n_hiddens_1 + 1) * n_hiddens_2 + (n_hiddens_2 + 1) * n_outputs\n",
    "w = (np.arange(n_w) - n_w/2) * 0.01\n",
    "# Y = network(w, n_inputs, n_hiddens_1, n_hiddens_2, n_outputs, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = network(w, n_inputs, n_hiddens_1, n_hiddens_2, n_outputs, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.12022254, 0.12773632, 0.13525011],\n",
       "       [0.08581156, 0.09176614, 0.09772072],\n",
       "       [0.05582873, 0.06042448, 0.06502022]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def error_gradient__06(w, n_inputs, n_hiddens, n_outputs, X, T):\n",
    "#    Y, Z = network(w, n_inputs, n_hiddens, n_outputs, X, all_outputs=True)\n",
    "#    n_samples = X.shape[0]\n",
    "#    delta_Y = -2 / n_samples * (T - Y)\n",
    "#    Z_hat = np.insert(Z, 0, 1, axis=1)\n",
    "#    dEdW = Z_hat.T @ delta_Y\n",
    "#    \n",
    "#    n_W = (n_hiddens + 1) * n_outputs\n",
    "#    W = w[-n_W:].reshape((n_hiddens + 1, n_outputs))\n",
    "#    delta_Z = (delta_Y @ W[1:, :].T) * (1 - Z**2)\n",
    "#    X_hat = np.insert(X, 0, 1, axis=1)\n",
    "#    dEdV = X_hat.T @ delta_Z\n",
    "# \n",
    "#    dEdw = np.hstack((dEdV.flatten(), dEdW.flatten()))\n",
    "#\n",
    "#    return dEdw\n",
    "def error_gradient(w, n_inputs, n_hiddens_1, n_hiddens_2, n_outputs, X, T):\n",
    "    Y, Z = network(w, n_inputs, n_hiddens, n_outputs, X, all_outputs=True)\n",
    "    n_samples = X.shape[0]\n",
    "    delta_Y = -2 / n_samples * (T - Y)\n",
    "    Z_hat = np.insert(Z, 0, 1, axis=1)\n",
    "    dEdW = Z_hat.T @ delta_Y\n",
    "    \n",
    "    n_W = (n_hiddens + 1) * n_outputs\n",
    "    W = w[-n_W:].reshape((n_hiddens + 1, n_outputs))\n",
    "    delta_Z = (delta_Y @ W[1:, :].T) * (1 - Z**2)\n",
    "    X_hat = np.insert(X, 0, 1, axis=1)\n",
    "    dEdV = X_hat.T @ delta_Z\n",
    " \n",
    "    dEdw = np.hstack((dEdV.flatten(), dEdW.flatten()))\n",
    "\n",
    "    return dEdw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def mse__06(w, n_inputs, n_hiddens, n_outputs, X, T):\n",
    "#    Y = network(w, n_inputs, n_hiddens, n_outputs, X)\n",
    "#    return np.mean((T - Y)**2)\n",
    "# THIS SHOULD BE GOOD TO GO\n",
    "def mse(w, n_inputs, n_hiddens_1, n_hiddens_2, n_outputs, X, T):\n",
    "    Y =  network(w, n_inputs, n_hiddens_1, n_hiddens_2, n_outputs, X)\n",
    "    return np.mean((T - Y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Air Quality Data\n",
    "Use your code to, as before, predict CO from the Hour of the day.\n",
    "\n",
    "Set up the data matrices `X` and `T` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas\n",
    "import optimizers as opt  # from Lecture Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pandas.read_csv('AirQualityUCI.csv', delimiter=';', decimal=',', usecols=range(15), na_values=-200)\n",
    "data = data[['Time', 'CO(GT)']]\n",
    "data = data [:23 * 20]  # first 20 days of data\n",
    "data = data.dropna(axis=0)\n",
    "print('data.shape =', data.shape)\n",
    "\n",
    "hour = [int(t[:2]) for t in data['Time']]\n",
    "X = np.array(hour).reshape(-1, 1)\n",
    "CO = data['CO(GT)']\n",
    "T = np.array(CO).reshape(-1, 1)\n",
    "np.hstack((X, T))[:10]  # show the first 10 samples of hour, CO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the network parameters.  Use the shapes of `X` and `T` to assign the number of inputs and outputs.  Define each of the two hidden layers to have 5 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = X.shape[1]\n",
    "n_hiddens_1 = 5\n",
    "n_hiddens_2 = 5\n",
    "n_outputs = T.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must define the intial weight vector.  The vector contains a value for each weight in all three layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_U = (n_inputs + 1) * n_hiddens_1\n",
    "n_V = (n_hiddens_1 + 1) * n_hiddens_2\n",
    "n_W = (n_hiddens_2 + 1) * n_outputs\n",
    "\n",
    "initial_w = np.random.uniform(-0.1, 0.1, n_U + n_V + n_W)  # range of weights is -0.1 to 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training our network, standardize the input values, to change the hour to have zero mean and unit variance across the set of samples.  Change `True` to `False` to not perform this step, allowing you to compare results with and without standardization.  It is not required to show the results here.  When you check in your notebook, leave `standardize` set to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardize = True\n",
    "\n",
    "if standardize:\n",
    "    X_mean = X.mean(axis=0)\n",
    "    X_std = X.std(axis=0)\n",
    "    \n",
    "    X = (X - X_mean) / X_std\n",
    "    \n",
    "print(f'X mean is {X.mean(axis=0)[0]:.3f} and its standard deviation is {X.std(axis=0)[0]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train our network using each of our three optimization algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 2000\n",
    "\n",
    "result_sgd = opt.sgd(initial_w,\n",
    "                     mse, error_gradient, fargs=[n_inputs, n_hiddens_1, n_hiddens_2, n_outputs, X, T],\n",
    "                     n_iterations=n_iterations, learning_rate=1e-1, momentum_rate=0.2, \n",
    "                     save_wtrace=True)\n",
    "#print(f'SGD final error is {result_sgd[\"ftrace\"][-1]:.3f} and it took {result_sgd[\"time\"]:.2f} seconds')\n",
    "\n",
    "#result_adam = opt.adam(initial_w, \n",
    "#                       mse, error_gradient, fargs=[n_inputs, n_hiddens_1, n_hiddens_2, n_outputs, X, T],\n",
    "#                       n_iterations=n_iterations, learning_rate=1e-2, \n",
    "#                       save_wtrace=True)\n",
    "#print(f'Adam final error is {result_adam[\"ftrace\"][-1]:.3f} and it took {result_adam[\"time\"]:.2f} seconds')\n",
    "\n",
    "#result_scg = opt.scg(initial_w,\n",
    "#                     mse, error_gradient, fargs=[n_inputs, n_hiddens_1, n_hiddens_2, n_outputs, X, T],\n",
    "#                     n_iterations=n_iterations,\n",
    "#                     save_wtrace=True)\n",
    "#print(f'SCG final error is {result_scg[\"ftrace\"][-1]:.3f} and it took {result_scg[\"time\"]:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the error curve and the model fits for each of the optimization algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(result_sgd['ftrace'], label='SGD')\n",
    "plt.plot(result_adam['ftrace'], label='Adam')\n",
    "plt.plot(result_scg['ftrace'], label='SCG')\n",
    "plt.legend()\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('MSE')\n",
    "plt.ylim(0, 4)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "if standardize:\n",
    "    plt.plot(X * X_std + X_mean, T, 'k.')  # unstandardize X\n",
    "else:\n",
    "    plt.plot(X, T, 'k.')\n",
    "xs = np.linspace(0, 23, 100).reshape((-1, 1))\n",
    "xs_standardized = (xs - X_mean) / X_std if standardize else xs\n",
    "plt.plot(xs, network(result_sgd['w'], n_inputs, n_hiddens_1, n_hiddens_2, n_outputs, xs_standardized), label='SGD')\n",
    "plt.plot(xs, network(result_adam['w'], n_inputs, n_hiddens_1, n_hiddens_2, n_outputs, xs_standardized), label='Adam')\n",
    "plt.plot(xs, network(result_scg['w'], n_inputs, n_hiddens_1, n_hiddens_2, n_outputs, xs_standardized), label='SCG')\n",
    "plt.legend()\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('CO');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some code that tries a number of values for the key parameters of `n_iterations`, `n_hiddens_1`, `n_hiddens_2` and `learning_rate`.\n",
    "\n",
    "**Required:** Modify the lists of values in the four for loops to try other parameter values.  Try to find ranges that work well for all three algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for n_iterations in [10, 100]:\n",
    "    for nh1 in [1, 2]:\n",
    "        for nh2 in [1, 2]:\n",
    "            \n",
    "            n_U = (n_inputs + 1) * nh1\n",
    "            n_V = (nh1 + 1) * nh2\n",
    "            n_W = (nh2 + 1) * n_outputs\n",
    "            initial_w = np.random.uniform(-0.1, 0.1, n_U + n_V + n_W)\n",
    "\n",
    "            result_scg = opt.scg(initial_w, mse, error_gradient, fargs=[n_inputs, nh1, nh2, n_outputs, X, T],\n",
    "                                 n_iterations=n_iterations)\n",
    "            results.append([n_iterations, nh1, nh2, 0, 'scg', result_scg['ftrace'][-1]])\n",
    "            \n",
    "            for lr in [1e-3, 1e-5]:\n",
    "                \n",
    "                result_sgd = opt.sgd(initial_w, mse, error_gradient, fargs=[n_inputs, nh1, nh2, n_outputs, X, T],\n",
    "                     n_iterations=n_iterations, learning_rate=lr, momentum_rate=0)\n",
    "                result_adam = opt.adam(initial_w, mse, error_gradient, fargs=[n_inputs, nh1, nh2, n_outputs, X, T],\n",
    "                                       n_iterations=n_iterations, learning_rate=lr)\n",
    "                results.append([n_iterations, nh1, nh2, 'sgd', result_scg['ftrace'][-1]])\n",
    "\n",
    "                results.append([n_iterations, nh1, nh2, lr, 'sgd', result_sgd['ftrace'][-1]])\n",
    "                results.append([n_iterations, nh1, nh2, lr, 'adam', result_adam['ftrace'][-1]])\n",
    "\n",
    "results = pandas.DataFrame(results, columns=('Iterations', 'nh1', 'nh2', 'lr', 'algo', 'mse'))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Required:** Show the results for the 20 lowest MSE values, sorted by increasing MSE. Read about the `sort_values` and `head` methods on a `DataFrame`.  You can do this with a single line of python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...code here....  Can be just one line of python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you have cleverly written your code to handle any value of $I$, $H_1$, $H_2$, and $K$, your code should be able to handle the following data that contains three attributes for each of 5 samples and two output values for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(15).reshape((5, 3))\n",
    "T = np.hstack((X[:, 0:1] * 0.1 * X[:, 1:2], X[:, 2:]**2)) # making two target values for each sample\n",
    "T = T.reshape((5, 2))\n",
    "print('  Input            Target')\n",
    "for x, t in zip(X, T):\n",
    "    print(x, '\\t', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use your code to train a neural network with two hidden layers, having 50 units in the first hidden layer and 3 units in the second hidden layer.  This example does not perform the standardization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hiddens_1 = 50\n",
    "n_hiddens_2 = 3\n",
    "n_iterations = 1000\n",
    "\n",
    "\n",
    "n_inputs = X.shape[1]\n",
    "n_outputs = T.shape[1]\n",
    "\n",
    "n_U = (n_inputs + 1) * n_hiddens_1\n",
    "n_V = (n_hiddens_1 + 1) * n_hiddens_2\n",
    "n_W = (n_hiddens_2 + 1) * n_outputs\n",
    "\n",
    "initial_w = np.random.uniform(-0.1, 0.1, n_U + n_V + n_W)  # range of weights is -0.1 to 0.1\n",
    "\n",
    "result_sgd = opt.sgd(initial_w,\n",
    "                     mse, error_gradient, fargs=[n_inputs, n_hiddens_1, n_hiddens_2, n_outputs, X, T],\n",
    "                     n_iterations=n_iterations, learning_rate=1e-1, momentum_rate=0.2, \n",
    "                     save_wtrace=True)\n",
    "print(f'SGD final error is {result_sgd[\"ftrace\"][-1]:.3f} and it took {result_sgd[\"time\"]:.2f} seconds')\n",
    "\n",
    "result_adam = opt.adam(initial_w, \n",
    "                       mse, error_gradient, fargs=[n_inputs, n_hiddens_1, n_hiddens_2, n_outputs, X, T],\n",
    "                       n_iterations=n_iterations, learning_rate=1e-2, \n",
    "                       save_wtrace=True)\n",
    "print(f'Adam final error is {result_adam[\"ftrace\"][-1]:.3f} and it took {result_adam[\"time\"]:.2f} seconds')\n",
    "\n",
    "result_scg = opt.scg(initial_w,\n",
    "                     mse, error_gradient, fargs=[n_inputs, n_hiddens_1, n_hiddens_2, n_outputs, X, T],\n",
    "                     n_iterations=n_iterations,\n",
    "                     save_wtrace=True)\n",
    "print(f'SCG final error is {result_scg[\"ftrace\"][-1]:.3f} and it took {result_scg[\"time\"]:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = result_scg['w']\n",
    "\n",
    "Y = network(w, n_inputs, n_hiddens_1, n_hiddens_2, n_outputs, X)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_diagonal(T, Y):\n",
    "    a = min(T.min(), Y.min())\n",
    "    b = max(T.max(), Y.max())\n",
    "    plt.plot([a, b], [a, b], '-', lw=3, alpha=0.5)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(result_scg['ftrace'])\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('SCG')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(T[:, 0], Y[:, 0], '.')\n",
    "plot_diagonal(T[:, 0], Y[:, 0])\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Prediction')\n",
    "plt.title('$Y_1$')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(T[:, 1], Y[:, 1], '.')\n",
    "plot_diagonal(T[:, 1], Y[: 1])\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Prediction')\n",
    "plt.title('$Y_2$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading\n",
    "\n",
    "Your notebook will be run and graded automatically. Test this grading process by first downloading [A2grader.zip](http://www.cs.colostate.edu/~anderson/cs545/notebooks/A2grader.zip) and extract `A2grader.py` from it. Run the code in the following cell to demonstrate an example grading session. You should see a perfect execution score of 80/80 if your functions are defined correctly. The remaining 20 points will be based on other testing and the results you obtain and your discussions. \n",
    "\n",
    "A different, but similar, grading script will be used to grade your checked-in notebook. It will include additional tests. You should design and perform additional tests on all of your functions to be sure they run correctly before checking in your notebook.  \n",
    "\n",
    "For the grading script to run correctly, you must first name this notebook as 'Lastname-A2.ipynb' with 'Lastname' being your last name, and then save this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================= Code Execution =======================\n",
      "\n",
      "Extracting python code from notebook named 'Staab-A2.ipynb' and storing in notebookcode.py\n",
      "Removing all statements that are not function or class defs or import statements.\n",
      "\n",
      "Testing\n",
      "X = np.arange(3 * 4).reshape((3, 4)) * 0.1\n",
      "T = np.hstack(( np.sin(X[:, 0:1]) + X[:, 1:2],\n",
      "                X[:, 2:3] * -0.5,\n",
      "                X[:, 3:4] ** 2))\n",
      "n_inputs = X.shape[1]\n",
      "n_outputs = T.shape[1]\n",
      "n_hiddens_1 = 6\n",
      "n_hiddens_2 = 2\n",
      "n_w = (n_inputs + 1) * n_hiddens_1 + (n_hiddens_1 + 1) * n_hiddens_2 + (n_hiddens_2 + 1) * n_outputs\n",
      "w = (np.arange(n_w) - n_w/2) * 0.01\n",
      "Y = network(w, n_inputs, n_hiddens_1, n_hiddens_2, n_outputs, X)\n",
      "\n",
      "\n",
      "--- 20/20 points. Returned correct values.\n",
      "\n",
      "Testing\n",
      "Y, Z1, Z2 = network(w, n_inputs, n_hiddens_1, n_hiddens_2, n_outputs, X, all_outputs=True)\n",
      "\n",
      "\n",
      "--- 0/20 points. network raised the exception\n",
      "\n",
      "name 'Z' is not defined\n",
      "\n",
      "Testing\n",
      "grad = error_gradient(w, n_inputs, n_hiddens_1, n_hiddens_2, n_outputs, X, T)\n",
      "\n",
      "\n",
      "--- 0/20 points. network raised the exception\n",
      "\n",
      "name 'n_hiddens' is not defined\n",
      "\n",
      "Testing\n",
      "m = mse(w, n_inputs, n_hiddens_1, n_hiddens_2, n_outputs, X, T)\n",
      "\n",
      "\n",
      "--- 20/20 points. Returned correct values.\n",
      "\n",
      "A2 Execution Grade is 40 / 80\n",
      "\n",
      "Your final assignment grade will be based on other tests.  Run additional tests\n",
      "of your own design to check your functions before checking in this notebook.\n",
      "\n",
      "A2 FINAL GRADE is ___ / 100\n"
     ]
    }
   ],
   "source": [
    "%run -i A2grader.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check-In\n",
    "\n",
    "Do not include this section in your notebook.\n",
    "\n",
    "Name your notebook ```Lastname-A2.ipynb```.  Submit the file using the ```Assignment 2``` link on [Canvas](https://colostate.instructure.com/courses/86986)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit\n",
    "\n",
    "Train a neural network with two hidden layers with five units in each layer to predict CO from the Hour of the day.  After it is trained plot the outputs of the five units in the first hidden layer versus 100 floating point values of hour from 0 to 23 in one subplot of one figure.  Then, in another subplot plot the outputs of the five units in the second hidden layer.  In a third subplot, plot the predicted CO from the output of the neural network.  Arrange the subplots vertically.  Describe what you see in the plots."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
