{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A4.2 Convolutional Neural Networks\n",
    "Brent Staab\n",
    "\n",
    "*by Chuck Anderson for [CS545: Machine Learning](https://www.cs.colostate.edu/~anderson/cs545) at Colorado State University*\n",
    "\n",
    "* 4.2: Corrected the code in Cell 11. A4grader.zip updated.\n",
    "* 4.1: Lots of details added, including A4grader.zip\n",
    "\n",
    "In this assignment, you will finish writing this new class, named `NeuralNetwork_Convolutional` that uses the `torch.nn` module to construct neural networks with initial convolutional layers followed by fully connected layers to learn to classify images.  Your main job is to finish the `train` function, following steps shown in lecture notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network will have some number, possibly zero, layers of convolutional units having the `tanh` activation function.  These are followed by one or more layers with fully connected units with `tanh` activation functions.\n",
    "\n",
    "A tricky part of building this structure is determining the correct number of inputs to the first fully-connected layer.  This requires knowing the size of the output image generated by the last convolutional layer.  If this layer produces an output image that is $n\\times n$ and it has $m$ units, then the concatenation of all image values for all units into one vector to form the input to the fully-connected layer, the resulting vector will be $m \\cdot n^2$.\n",
    "\n",
    "As an example, say our data contains images that are $d\\times d$ and we have a neural network with two convolutional layers and one fully connected layer. \n",
    "\n",
    "Original input image: \n",
    "- $d\\times d$ of $c$ channels\n",
    "\n",
    "First convolutional layer with $u_1$ units having kernels sized $k_1\\times k_1$ and strides of $s_1$\n",
    "- input is $d\\times d$ of $c$ channels\n",
    "- output is $z_1\\times z_1$ of $u_1$ channels, where $z_1 = \\frac{d - k_1}{s_1} + 1$\n",
    "\n",
    "Second convolutional layer with $u_2$ units having kernels sized $k_2\\times k_2$ and strides of $s_2$\n",
    "- input is $z_1\\times z_1$ of $u_1$ channels\n",
    "- output is $z_2\\times z_2$ of $u_2$ channels, where $z_2 = \\frac{z_1 - k_2}{s_2} + 1$\n",
    "\n",
    "\n",
    "Third convolutional layer is fully-connected, so we must flatten into a vector the output from the previous convolutional layer. Say this layer has $u_3$ units.\n",
    "- input is $u_2 \\, z_2 ^ 2$\n",
    "- output is $u_3$ values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import code needed in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import copy\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pickle, gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of `NeuralNetwork_Convolutional`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork_Convolutional():\n",
    "    # Initilization function (a.k.a. constructor) for object\n",
    "    def __init__(self, \n",
    "                 n_channels_in_image,         # (int) number of values per pixel \n",
    "                 image_size,                  # (int) number of rows in image, same as number of columns\n",
    "                 n_units_in_conv_layers,      # (list of ints) number of units in each convolutional layer\n",
    "                 kernels_size_and_stride,     # (list of lists) each list is [kernel_size, kernel_stride] for each convolutional layer\n",
    "                 n_units_in_fc_hidden_layers, # (list of ints) number of units in fully-connected layers\n",
    "                 classes,                     # (list of ints) labels for each class\n",
    "                 use_gpu=False):              # (boolean) flag indicating GPU use or not (default = no GPU)\n",
    "\n",
    "        if not isinstance(n_units_in_conv_layers, list):\n",
    "            raise Exception('n_units_in_conv_layers must be a list')\n",
    "\n",
    "        if not isinstance(n_units_in_fc_hidden_layers, list):\n",
    "            raise Exception('n_units_in_fc_hidden_layers must be a list')\n",
    "        \n",
    "        if use_gpu and not torch.cuda.is_available():\n",
    "            print('\\nGPU is not available. Running on CPU.\\n')\n",
    "            use_gpu = False\n",
    "\n",
    "        self.n_channels_in_image = n_channels_in_image\n",
    "        self.image_size = image_size \n",
    "        self.n_units_in_conv_layers = n_units_in_conv_layers\n",
    "        self.n_units_in_fc_hidden_layers = n_units_in_fc_hidden_layers\n",
    "        self.kernels_size_and_stride = kernels_size_and_stride\n",
    "        self.n_outputs = len(classes)\n",
    "        self.classes = np.array(classes)\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "        self.n_conv_layers = len(self.n_units_in_conv_layers)\n",
    "        self.n_fc_hidden_layers = len(self.n_units_in_fc_hidden_layers)\n",
    "      \n",
    "        # Build the net layers\n",
    "        self.nnet = torch.nn.Sequential()\n",
    "\n",
    "        # Add convolutional layers\n",
    "        n_units_previous = self.n_channels_in_image\n",
    "        output_size_previous = self.image_size\n",
    "        n_layers = 0\n",
    "        if self.n_conv_layers > 0:\n",
    "\n",
    "            for (n_units, kernel) in zip(self.n_units_in_conv_layers, self.kernels_size_and_stride):\n",
    "                n_units_previous, output_size_previous = self._add_conv2d_tanh(n_layers,\n",
    "                                        n_units_previous, output_size_previous, n_units, kernel)\n",
    "                n_layers += 1 # for text label in layer\n",
    "                \n",
    "            self.nnet.add_module('flatten', torch.nn.Flatten())  # prepare for fc layers\n",
    "\n",
    "        n_inputs = output_size_previous ** 2 * n_units_previous\n",
    "        if self.n_fc_hidden_layers > 0:\n",
    "            for n_units in self.n_units_in_fc_hidden_layers:\n",
    "                n_inputs = self._add_fc_tanh(n_layers, n_inputs, n_units)\n",
    "                n_layers += 1\n",
    "\n",
    "        self.nnet.add_module(f'output_{n_layers}', torch.nn.Linear(n_inputs, self.n_outputs))\n",
    "        \n",
    "        ##self.loss_F = torch.nn.MSELoss()\n",
    "        self.loss_F = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        learning_rate = 0.01\n",
    "        self.optimizer = torch.optim.Adam(self.nnet.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Member variables for standardization\n",
    "        self.Xmeans = None\n",
    "        self.Xstds = None\n",
    "\n",
    "        if self.use_gpu:\n",
    "            self.nnet.cuda()\n",
    "\n",
    "        self.n_epochs = 0\n",
    "        self.error_trace = []\n",
    "\n",
    "    def _add_conv2d_tanh(self, n_layers, n_units_previous, output_size_previous,\n",
    "                   n_units, kernel_size_and_stride):\n",
    "        kernel_size, kernel_stride = kernel_size_and_stride\n",
    "        self.nnet.add_module(f'conv_{n_layers}', torch.nn.Conv2d(n_units_previous, n_units,\n",
    "                                                                 kernel_size, kernel_stride))\n",
    "        self.nnet.add_module(f'output_{n_layers}', torch.nn.Tanh())\n",
    "        output_size_previous = (output_size_previous - kernel_size) // kernel_stride + 1\n",
    "        n_units_previous = n_units                \n",
    "        return n_units_previous, output_size_previous\n",
    "    \n",
    "    def _add_fc_tanh(self, n_layers, n_inputs, n_units):\n",
    "        self.nnet.add_module(f'linear_{n_layers}', torch.nn.Linear(n_inputs, n_units))\n",
    "        self.nnet.add_module(f'output_{n_layers}', torch.nn.Tanh())\n",
    "        n_inputs = n_units\n",
    "        return n_inputs\n",
    "\n",
    "    def __repr__(self):\n",
    "        str = f'''{type(self).__name__}(\n",
    "                            n_channels_in_image={self.n_channels_in_image},\n",
    "                            image_size={self.image_size},\n",
    "                            n_units_in_conv_layers={self.n_units_in_conv_layers},\n",
    "                            kernels_size_and_stride={self.kernels_size_and_stride},\n",
    "                            n_units_in_fc_hidden_layers={self.n_units_in_fc_hidden_layers},\n",
    "                            classes={self.classes},\n",
    "                            use_gpu={self.use_gpu})'''\n",
    "\n",
    "        str += self.nnet\n",
    "        if self.n_epochs > 0:\n",
    "            str += f'\\n   Network was trained for {self.n_epochs} epochs that took {self.training_time:.4f} seconds.'\n",
    "            str += f'\\n   Final objective value is {self.error_trace[-1]:.3f}'\n",
    "        else:\n",
    "            str += '  Network is not trained.'\n",
    "        return str\n",
    "        \n",
    "    def _standardizeX(self, X):\n",
    "        result = (X - self.Xmeans) / self.XstdsFixed\n",
    "        result[:, self.Xconstant] = 0.0\n",
    "        return result\n",
    "\n",
    "    def _unstandardizeX(self, Xs):\n",
    "        return self.Xstds * Xs + self.Xmeans\n",
    "\n",
    "    def _setup_standardize(self, X, T):\n",
    "        if self.Xmeans is None:\n",
    "            self.Xmeans = X.mean(axis=0)\n",
    "            self.Xstds = X.std(axis=0)\n",
    "            self.Xconstant = self.Xstds == 0\n",
    "            self.XstdsFixed = copy.copy(self.Xstds)\n",
    "            self.XstdsFixed[self.Xconstant] = 1\n",
    "\n",
    "    def train(self, X, T, n_epochs, learning_rate=0.01):\n",
    "\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        if T.ndim == 1:\n",
    "            T = T.reshape((-1, 1))\n",
    "\n",
    "        _, T = np.where(T == self.classes)  # convert to labels from 0\n",
    "\n",
    "        self._setup_standardize(X, T)\n",
    "        X = self._standardizeX(X)\n",
    "\n",
    "        X = torch.tensor(X)\n",
    "        T = torch.tensor(T.reshape(-1))\n",
    "        if self.use_gpu:\n",
    "            X = X.cuda()\n",
    "            T = T.cuda()\n",
    "\n",
    "            \n",
    "        # You fill in the rest of the train function, following lecture notes example.\n",
    "        for epoch in range(n_epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            Y = self.nnet(X)\n",
    "\n",
    "            error = self.loss_F(Y, T)\n",
    "            if epoch % 5 == 0:\n",
    "                print(f'Epoch {epoch} error {error:.5f}')\n",
    "\n",
    "            error.backward()\n",
    "\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        self.training_time = time.time() - start_time\n",
    "        \n",
    "    def get_error_trace(self):\n",
    "        return self.error_trace\n",
    "    \n",
    "    def _softmax(self, Y):\n",
    "        mx = Y.max()\n",
    "        expY = np.exp(Y - mx)\n",
    "        denom = expY.sum(axis=1).reshape((-1, 1)) + sys.float_info.epsilon\n",
    "        return expY / denom\n",
    "    \n",
    "    def use(self, X):\n",
    "        self.nnet.eval()  # turn off gradients and other aspects of training\n",
    "        X = self._standardizeX(X)\n",
    "        X = torch.tensor(X)\n",
    "        if self.use_gpu:\n",
    "            X = X.cuda()\n",
    "\n",
    "        Y = self.nnet(X)\n",
    "\n",
    "        if self.use_gpu:\n",
    "            Y = Y.cpu()\n",
    "        Y = Y.detach().numpy()\n",
    "        Yclasses = self.classes[Y.argmax(axis=1)].reshape((-1, 1))\n",
    "\n",
    "        return Yclasses, self._softmax(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Example with Squares and Diamonds\n",
    "\n",
    "Repeating the example from lecture notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 1, 20, 20), (1000, 1), (20, 1, 20, 20), (20, 1))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_images(nEach):\n",
    "    images = np.zeros((nEach * 2, 1, 20, 20))  # nSamples, nChannels, rows, columns\n",
    "    radii = 3 + np.random.randint(10 - 5, size=(nEach * 2, 1))\n",
    "    centers = np.zeros((nEach * 2, 2))\n",
    "    for i in range(nEach * 2):\n",
    "        r = radii[i, 0]\n",
    "        centers[i, :] = r + 1 + np.random.randint(18 - 2 * r, size=(1, 2))\n",
    "        x = int(centers[i, 0])\n",
    "        y = int(centers[i, 1])\n",
    "        if i < nEach:\n",
    "            # squares\n",
    "            images[i, 0, x - r:x + r, y + r] = 1.0\n",
    "            images[i, 0, x - r:x + r, y - r] = 1.0\n",
    "            images[i, 0, x - r, y - r:y + r] = 1.0\n",
    "            images[i, 0, x + r, y - r:y + r + 1] = 1.0\n",
    "        else:\n",
    "            # diamonds\n",
    "            images[i, 0, range(x - r, x), range(y, y + r)] = 1.0\n",
    "            images[i, 0, range(x - r, x), range(y, y - r, -1)] = 1.0\n",
    "            images[i, 0, range(x, x + r + 1), range(y + r, y - 1, -1)] = 1.0\n",
    "            images[i, 0, range(x, x + r), range(y - r, y)] = 1.0\n",
    "            # images += np.random.randn(*images.shape) * 0.5\n",
    "            T = np.ones((nEach * 2, 1))\n",
    "            T[nEach:] = 2\n",
    "    return images.astype(np.float32), T.astype(np.int)\n",
    "\n",
    "Xtrain, Ttrain = make_images(500)\n",
    "Xtest, Ttest = make_images(10)\n",
    "\n",
    "Xtrain.shape, Ttrain.shape, Xtest.shape, Ttest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet = NeuralNetwork_Convolutional(n_channels_in_image=Xtrain.shape[1],\n",
    "                                   image_size=Xtrain.shape[2],\n",
    "                                   n_units_in_conv_layers=[5], # , 5],\n",
    "                                   n_units_in_fc_hidden_layers=[2], # 10, 10],\n",
    "                                   classes=[1, 2],\n",
    "                                   kernels_size_and_stride=[[5, 2]], #conv_kernels=[[5, 2]], # , [4, 1]],\n",
    "                                   use_gpu=False)\n",
    "\n",
    "#nnet.train(Xtrain, Ttrain, 50, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 error 0.69029\n",
      "Epoch 5 error 0.45521\n",
      "Epoch 10 error 0.35499\n",
      "Epoch 15 error 0.29423\n",
      "Epoch 20 error 0.24530\n",
      "Epoch 25 error 0.20069\n",
      "Epoch 30 error 0.15454\n",
      "Epoch 35 error 0.11443\n",
      "Epoch 40 error 0.08972\n",
      "Epoch 45 error 0.07461\n"
     ]
    }
   ],
   "source": [
    "nnet.train(Xtrain, Ttrain, 50, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nnet.get_error_trace())\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 out of 20 test samples correctly classified.  Training took 0.439 seconds.\n"
     ]
    }
   ],
   "source": [
    "Yclasses, Y = nnet.use(Xtest)\n",
    "\n",
    "print(f'{np.sum(Ttest == Yclasses)} out of {Ttest.shape[0]} test samples correctly classified.', end='')\n",
    "print(f'  Training took {nnet.training_time:.3f} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the output of the convolutional layer produces, and the weight matrices of each unit that produce those output images.   Here are functions, `show_layer_output` and `show_layer_weights` that will do this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_layer_output(nnet, X_sample, layer):\n",
    "    outputs = []\n",
    "    reg = nnet.nnet[layer * 2].register_forward_hook(\n",
    "        lambda self, i, o: outputs.append(o))\n",
    "    nnet.use(X_sample)\n",
    "    reg.remove()\n",
    "    output = outputs[0]\n",
    "\n",
    "    n_units = output.shape[1]\n",
    "    nplots = int(np.sqrt(n_units)) + 1\n",
    "    for unit in range(n_units):\n",
    "        plt.subplot(nplots, nplots, unit+1)\n",
    "        plt.imshow(output[0, unit, :, :].detach(),cmap='binary')\n",
    "        plt.axis('off')\n",
    "    return output\n",
    "\n",
    "def show_layer_weights(nnet, layer):\n",
    "    W = nnet.nnet[layer*2].weight.detach()\n",
    "    n_units = W.shape[0]\n",
    "    nplots = int(np.sqrt(n_units)) + 1\n",
    "    for unit in range(n_units):\n",
    "        plt.subplot(nplots, nplots, unit + 1)\n",
    "        plt.imshow(W[unit, 0, :, :], cmap='binary')\n",
    "        plt.axis('off')\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 19.5, 19.5, -0.5)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAC8ElEQVR4nO3bQYrDQAwAwdXi/3958oA4mICTaULV0b7o0giMPGutP6Dnf/cAwDlxQpQ4IUqcECVOiDou3vuUC583Zw9tTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRF2d7203c3rZxI/ws/9rNidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6Ly53vvcArW4OTyHjYnRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRB27B7jTzOweAW5jc0KUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4ISp/vrfW2j0CbGFzQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROijov385UpgCc2J0SJE6LECVHihChxQpQ4IeoBfyAO2BHAUtkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_sample = Xtest[0:1, :, :, :]\n",
    "plt.imshow(X_sample[0, 0, :, :], cmap='binary')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_layer_output(nnet, X_sample, 0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_layer_weights(nnet, 0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Digits\n",
    "\n",
    "Investigate the application of your code to the classification of MNIST digits, which you may download from [this site](http://deeplearning.net/tutorial/gettingstarted.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle, gzip\n",
    "\n",
    "# Load the dataset\n",
    "with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 784), (50000,))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain = train_set[0]\n",
    "Ttrain = train_set[1]\n",
    "Xtrain.shape, Ttrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Xtrain[0, :].reshape(28, 28)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.imshow(Xtrain[i, :].reshape(28, 28), cmap='binary')\n",
    "    plt.title(Ttrain[i])\n",
    "    plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Required:** Try different numbers of layers and units, kernel sizes and strides, and learning rates and epochs to get greater than 90% test images correctly classified.  Your grade will only depend on you showing various results, not on achieving this accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grading\n",
    "\n",
    "Download [A4grader.zip](https://www.cs.colostate.edu/~anderson/cs545/notebooks/A4grader.zip) and extract A4grader.py from it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================= Code Execution =======================\n",
      "\n",
      "Extracting python code from notebook named 'Anderson-A4.ipynb' and storing in notebookcode.py\n",
      "Removing all statements that are not function or class defs or import statements.\n",
      "\n",
      "Testing if your NeuralNetwork_Convolutional can learn to classify a small \n",
      "subset of hand_drawn 0, 1, and 2 digits.\n",
      "\n",
      "import numpy as np\n",
      "import pickle, gzip\n",
      "\n",
      "# Load the dataset\n",
      "with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
      "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
      "Xtest = test_set[0]\n",
      "traini = [3,  10,  13,  25,  28,  55,  69,  71, 101, 126, 2,   5,  14,  29,  31,  37,  39,  40,  46,  57, 1,  35,  38,  43,  47,  72,  77,  82, 106, 119]\n",
      "testi = [136, 148, 157, 183, 188, 192, 194, 215, 246, 269,  74,  89,  94, 96, 107, 135, 137, 143, 145, 154, 147, 149, 172, 174, 186, 199, 208, 221, 222, 225]\n",
      "Xtrain = test_set[0][traini, :].reshape(-1, 1, 28, 28)\n",
      "Ttrain = test_set[1][traini].reshape(-1, 1)\n",
      "Xtest = test_set[0][testi, :].reshape(-1, 1, 28, 28)\n",
      "Ttest = test_set[1][testi].reshape(-1, 1)\n",
      "\n",
      "torch.random.manual_seed(42)\n",
      "\n",
      "nnet = NeuralNetwork_Convolutional(n_channels_in_image=Xtrain.shape[1],\n",
      "                                   image_size=Xtrain.shape[2],\n",
      "                                   n_units_in_conv_layers=[5, 5],\n",
      "                                   conv_kernels=[[5, 3], [4, 2]],\n",
      "                                   n_units_in_fc_hidden_layers=[10],\n",
      "                                   classes=[0, 1, 2],\n",
      "                                   use_gpu=False)\n",
      "\n",
      "nnet.train(Xtrain, Ttrain, 20, learning_rate=0.01)\n",
      "Yclasses, Y = nnet.use(Xtest)\n",
      "n_correct = (Yclasses == Ttest).sum()\n",
      "print(f'{n_correct} out of {Ttest.shape[0]} samples, or {n_correct/Ttest.shape[0]*100:.2f} percent.')\n",
      "\n",
      "Epoch 2 error 1.03157\n",
      "Epoch 4 error 0.87973\n",
      "Epoch 6 error 0.69274\n",
      "Epoch 8 error 0.51123\n",
      "Epoch 10 error 0.37162\n",
      "Epoch 12 error 0.27101\n",
      "Epoch 14 error 0.20077\n",
      "Epoch 16 error 0.15003\n",
      "Epoch 18 error 0.11277\n",
      "Epoch 20 error 0.08578\n",
      "27 out of 30 samples, or 90.00 percent.\n",
      "\n",
      "--- 80/80 points. Returned correct value of 27.\n",
      "\n",
      "Testing \n",
      "errors = nnet.get_error_trace()\n",
      "\n",
      "\n",
      "--- 10/10 points. Returned correct number of errors in error_trace.\n",
      "\n",
      "======================================================================\n",
      "A4 Execution Grade is 90 / 90\n",
      "======================================================================\n",
      "\n",
      " __ / 10 You show results for at least 10 different runs having various\n",
      "values for number of layers, units, and epochs for modeling MNIST data.\n",
      "You must show percent correct for train, validation and test sets for each run.\n",
      "Discuss your results. \n",
      "\n",
      "======================================================================\n",
      "A4 FINAL GRADE is  _  / 100\n",
      "======================================================================\n",
      "\n",
      "Extra Credit:\n",
      "1. For one of your runs, display the output images of your convolutional layers \n",
      "   and the weights for those layers.  Discuss what you see.  Describe why the\n",
      "   displayed weight patterns result in the output images of the first convolutional\n",
      "   layer.\n",
      "2. Make at least one of your runs on a workstation with a GPU and run with use_gpu=True.\n",
      "   Also run it with use_gpu=False.  Discuss the differences in training times.\n",
      "\n",
      "A4 EXTRA CREDIT is 0 / 2\n"
     ]
    }
   ],
   "source": [
    "%run -i A4grader.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
