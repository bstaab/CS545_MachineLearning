{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3 Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brent Staab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "In this assignment you will define a new class named `NeuralNetworkClassifier` that extends the given class `NeuralNetwork`.  Your new class will reuse most of the code in its parent class, but you will have to define the parts of the code that differ to allow the network to do classification. \n",
    "\n",
    "To learn a bit about how to call methods in a parent class, take a look at this\n",
    "[introduction to inheritance](https://realpython.com/python-super/) and examples of using `super()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by downloading [A3code.zip](https://www.cs.colostate.edu/~anderson/cs545/notebooks/A3code.zip).  Unzip this and you should see these files:\n",
    "\n",
    "*  `mlutilities.py`\n",
    "*  `neuralnetworks.py`\n",
    "*  `optimizers.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import code needed in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import copy\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import mlutilities as ml  # for ml.draw\n",
    "import optimizers as opt  # for opt.sgd, opt.adam, and opt.scg\n",
    "\n",
    "import neuralnetworks as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of `NeuralNetworkClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend the nn.NeuralNetwork class to reuse much of its implementation\n",
    "# Only those methods that must be altered to do classification are defined in NeuralNetworkClassifier\n",
    "class NeuralNetworkClassifier(nn.NeuralNetwork):\n",
    "\n",
    "    # Constructor\n",
    "    # Input parameters:\n",
    "    #    n_inputs: number of inputs\n",
    "    #    n_hiddens_list: a list containing the number of hidden layers\n",
    "    #    classes       : a list of classes (integer) to classify\n",
    "    #    use_torch     : flag indicating which library to use for the implementation\n",
    "    #                    False (default) - Use numpy\n",
    "    #                    True - Use pytorchc\n",
    "    def __init__(self, n_inputs, n_hiddens_list, classes, use_torch=False):\n",
    "\n",
    "        # Force n_hidens_list to be a list\n",
    "        if not isinstance(n_hiddens_list, list):\n",
    "            raise Exception('NeuralNetworkClassifier: n_hiddens_list must be a list.')\n",
    " \n",
    "        # Call the constructor for NeuralNetwork, passing in the number of unique class names (ints)\n",
    "        # as the number of outputs\n",
    "        super().__init__(n_inputs, n_hiddens_list, len(classes), use_torch)\n",
    "\n",
    "        # Store as member variables other things needed by instances of this class.\n",
    "        self.classes = np.array(classes) # to allow argmax in use()\n",
    "        \n",
    "        # Set functions used by the class based on 'use_torch' flag\n",
    "        if use_torch:\n",
    "            self.log = torch.log\n",
    "            self.exp = torch.exp\n",
    "            self.sum = torch.sum\n",
    "        else:\n",
    "            self.log = np.log\n",
    "            self.exp = np.exp\n",
    "            self.sum = np.sum\n",
    "\n",
    "    # built-in function used to compute the \"official\" string representation for this object\n",
    "    def __repr__(self):\n",
    "        str = f'{type(self).__name__}({self.n_inputs}, {self.n_hiddens_list}, {self.classes}, use_torch={self.use_torch})'\n",
    "        if self.trained:\n",
    "            str += f'\\n   Network was trained for {self.n_epochs} epochs'\n",
    "            str += f' that took {self.training_time:.4f} seconds. Final objective value is {self.error_trace[-1]:.3f}'\n",
    "        else:\n",
    "            str += '  Network is not trained.'\n",
    "        return str\n",
    "    \n",
    "    # Override base class implementation to do nothing\n",
    "    def _standardizeT(self, T):\n",
    "        return T\n",
    "    \n",
    "    # Override base class implementation to do nothing\n",
    "    def _unstandardizeT(self, T):\n",
    "        return T\n",
    "    \n",
    "    # Override base class implementation to add classification specific \n",
    "    # behavior.  Just call g(s) (a.k.a. softmax) on base class result\n",
    "    # Input\n",
    "    #    X: input data\n",
    "    # Output\n",
    "    #    Y: output of neural network\n",
    "    #    Z: the inputs, as a list, for each layer\n",
    "    def _forward_pass(self, X):\n",
    "        Y, Z = super()._forward_pass(X)\n",
    "        Y = self._g(Y)\n",
    "        \n",
    "        return Y, Z\n",
    "\n",
    "    # Override base class implementation\n",
    "    # Return neg_mean_log_likelihood\n",
    "    # Input\n",
    "    #    w: Weights to use for nn\n",
    "    #    X: Input data to process\n",
    "    #    T: Targets\n",
    "    # Output\n",
    "    #    neg_mean_log_likelihood\n",
    "    def _objectiveF(self, w, X, T):\n",
    "        self._unpack(w)\n",
    "        Y, _ = self._forward_pass(X)\n",
    "        return -np.mean(np.log(Y) * T)\n",
    "        \n",
    "    # Override base class implementation\n",
    "    # Input\n",
    "    #    neg_mean_log_likelihood: Output of _objectiveF\n",
    "    def _objective_to_actual(self, neg_mean_log_likelihood):\n",
    "        return self.exp(- neg_mean_log_likelihood)\n",
    "    \n",
    "    # Override base class implementation\n",
    "    # Return neg_mean_log_likelihood\n",
    "    # Input\n",
    "    #    X                    : Input data to process\n",
    "    #    T                    : Targets\n",
    "    #    n_epochs             : number of max num iterations to train\n",
    "    #    method               : Which algorithm (scg, sgd, adam) to use for training\n",
    "    #    verbose              : flag (default false) to enable/disable extra messages\n",
    "    #    save_weights_history : flag (default false) to enable/disable saving of the weights\n",
    "    #    learning_rate        : value used for sgd and adam\n",
    "    #    momentum_rate        : value used for sgd and adam\n",
    "    def train(self, X, T, n_epochs, method='scg',\n",
    "              verbose=False, save_weights_history=False,\n",
    "              learning_rate=0.001, momentum_rate=0.0):\n",
    "        \n",
    "        # Convert targets into indicator variables\n",
    "        T_indicator_vars = self._make_indicator_variables(T)\n",
    "        \n",
    "        # Call base class with indicator variables instead of target values\n",
    "        return super().train(X, T_indicator_vars, n_epochs, method,\n",
    "                          verbose, save_weights_history,\n",
    "                          learning_rate, momentum_rate)\n",
    "    \n",
    "    # Override base class implementation\n",
    "    # Return output of trained network on input data\n",
    "    # Input\n",
    "    #    X           : Input data to process\n",
    "    #    all_outputs : Flag (default false) to control what is returned from this function\n",
    "    def use(self, X, all_outputs=False):\n",
    "        if self.use_torch:\n",
    "            if not isinstance(X, torch.Tensor):\n",
    "                X = torch.tensor(X, dtype=torch.float)\n",
    "        X = self._standardizeX(X)\n",
    "        Y, Z = self._forward_pass(X)\n",
    "        Y = self._unstandardizeT(Y)\n",
    "        \n",
    "        # Convert Y to 'classes' \n",
    "        # - argmax returns an index into the class array, not the value\n",
    "        ##Y_classes = np.argmax(Y,axis=1).reshape((-1, 1))\n",
    "        Y_class_idx = list(np.argmax(Y, axis=1))\n",
    "        Y_classes = np.array([self.classes[i] for i in Y_class_idx]).reshape((-1,1))\n",
    "        \n",
    "        if self.use_torch:\n",
    "            Y = Y.detach().cpu().numpy()\n",
    "            Z = [Zi.detach().cpu().numpy() for Zi in Z]\n",
    "        \n",
    "        return (Y_classes, Y, Z[1:]) if all_outputs else Y_classes, Y\n",
    "        \n",
    "    # Function to convert a list of target values into an array \n",
    "    # of 'indicator variables'\n",
    "    def _make_indicator_variables(self, T):\n",
    "        # Make sure T is two-dimensiona. Should be nSamples x 1.\n",
    "        if T.ndim == 1:\n",
    "            T = T.reshape((-1,1))    \n",
    "        return (T == np.unique(T)).astype(int)\n",
    "    \n",
    "    # Function to compute the 'softmax'\n",
    "    def _g(self, Y):\n",
    "        fs = self.exp(Y)\n",
    "        denom = self.sum(fs, axis=1).reshape((-1,1))\n",
    "        gs = fs / denom\n",
    "        return gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetworkClassifier(2, [10, 10], [0 1], use_torch=True)  Network is not trained."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)  # Only used to commpare with lecture notes\n",
    "\n",
    "nnet_new = NeuralNetworkClassifier(2, [10, 10], [0, 1], True)\n",
    "nnet_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When thet class is printed, the '\\__repr__' functioin is called.  It shows the parameters passed in to the constructor and the fact that it hasn't been trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward_pass requires input to be a tensor object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xst = torch.tensor(X, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y, Z = nnet_new._forward_pass(Xst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)  # only to help you compare your output to mine.  Do not use otherwise.\n",
    "\n",
    "n_samples = 20\n",
    "X = np.random.choice(3, (n_samples, 2))\n",
    "T = (X[:, 0:1] == X[:, 1:2]).astype(int)  # where the two inputs are equal\n",
    "classes = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_std = nnet_new._standardizeT(T)\n",
    "n_diff = (T_std != T).sum()\n",
    "print(f'There are {n_diff} samples different')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_unstd = nnet_new._unstandardizeT(T)\n",
    "n_diff = (T_unstd != T).sum()\n",
    "print(f'There are {n_diff} samples different')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two code cells above (9 & 10) show that the '_standardizeT' and '_unstandardizeT' have no effect.  The output of each function is saved, then compareed with the input.  The total number of differences are printed and show that they are identical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standardize the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet_new._setup_standardize(X, T)\n",
    "Xst = nnet_new._standardizeX(X)\n",
    "Xst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the derived class does not implement these functions, the basse class functions will be called.  Also, we are using the same data provided in A3 notebook and we get the same result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Verify  _forward_pass( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y, Z = nnet_new._forward_pass(Xst)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _forward_pass function overrides the base class implementation.  The main difference is that the output of this function is the 'softmax'.  Since we are using the same input data, we are getting the same values from A3 notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Verify _objectiveF( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w = nnet_new._pack(nnet_new.Vs, nnet_new.W)\n",
    "\n",
    "T_indicator_vars = nnet_new._make_indicator_variables(T)\n",
    "\n",
    "nnet_new._objectiveF(w, X, T_indicator_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are converting the target data into a 'one-hot' representation and passing that to the _objectiveF function.  The objective function is returning the neg_mean_log_likelihood, and since we are using the same values from A3, we are getting the same expected value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Verify use( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_classes, Y = nnet_new.use(X)\n",
    "Y_classes.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows the output of the neural network 'use( )' function.  Given that we have 20 input samples, the shapes are expected.  The classes is 20x1 which means each of the input samples have been resolved to a specific class.  The Y output is 20x2, which again makes sense as it shows the probability that the input belongs to each of the target variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train the network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet_new.train(X, T, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is calling the train function and '\\__repr__' is used to print.  It shows the configuration of the object (parameters passed in) and that the network was trained.  Also, since it's trained, it shows the training information and the most important part is the final objective value = 1.000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use the trained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_classes, Y = nnet_new.use(X)\n",
    "Y_classes.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the neural network is trained, feed it test data so it can be printed in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Error Trace')\n",
    "plt.plot(nnet_new.get_error_trace())\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Likelihood')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Actual and Expected')\n",
    "plt.plot(T, 'o-')\n",
    "plt.plot(Y_classes + 0.05, 'o-')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Class')\n",
    "plt.legend(['Target Class', 'Predicted Class']);\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot on the left shows the log likelihool vs iteration which is the output of the training function.  It shows that the network needs about 35 intervals to reach the best behavior.  The plot on the right shows the actual data ('Target class') and the values predicted by the trained neural network ('Predicted Class').  Since the predicted values exatly match the expected values, a small offset was added so you could clearly see both sets of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using the [Wine](https://archive.ics.uci.edu/ml/datasets/Wine) classification data from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets.php).  This data is the result of a chemical analysis of wines grown in the same region in Italy, but derived from three different cultivars.  There are thirteen data points for each sample, and the objective is to see if you can find which cultivar provided the wine based on this analysis.\n",
    "\n",
    "The wines are resolved into one of three classes which represent the cultivars.  This is the 1st attribute of the data.  \n",
    "- Number of instances  \n",
    "  - class 1 59  \n",
    "  - class 2 71  \n",
    "  - class 3 48  \n",
    "- TOTAL = 178"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from the CSV file\n",
    "data = np.loadtxt('wine.data', delimiter=\",\")\n",
    "\n",
    "# NOTE: The data is ordered based on the resulting class.  For the initial experiment, \n",
    "#       I will leave it as is, just because I think the charts are easier to understand.\n",
    "#       You can uncomment the following line to randomize the data if desired\n",
    "#np.random.shuffle(data)\n",
    "\n",
    "# The first column is the class\n",
    "# Take all rows, first column\n",
    "T = data[:,0].reshape(-1,1).astype(int)\n",
    "\n",
    "# Get class list from the Target Data\n",
    "class_list = list(np.unique(T))\n",
    "\n",
    "# Data is everything except first column\n",
    "# Take all rows, all but first colum\n",
    "X = data[:,1:]\n",
    "\n",
    "# Get the number of inputs from the data\n",
    "num_inputs = X.shape[1]\n",
    "\n",
    "# Print some usefull info\n",
    "print(f'X.shape({X.shape}), T.shape({T.shape}))')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'scg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try a different number of hidden layers and units in the hidden layers with the 'scg' algorithm.  The learning rate and momentum rate do not apply to this algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "net_test_0, Y_class_test_0 = run(num_inputs,           [], class_list, X, T, X, 30, 'scg', 0.001, 0.0)\n",
    "net_test_1, Y_class_test_1 = run(num_inputs,         [50], class_list, X, T, X, 30, 'scg', 0.001, 0.0)\n",
    "net_test_2, Y_class_test_2 = run(num_inputs, [50, 20, 20], class_list, X, T, X, 30, 'scg', 0.001, 0.0)\n",
    "n_correct_0 = (Y_class_test_0 == T).sum()\n",
    "n_correct_1 = (Y_class_test_1 == T).sum()\n",
    "n_correct_2 = (Y_class_test_2 == T).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "summary(net_test_0, net_test_1, net_test_2, T, Y_class_test_0, n_correct_0, n_correct_1, n_correct_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network trains better with fewer hidden layers, but all configurations train within 15 iterations.  Once trained, all network correctly predict 100% of the datapoints.  The learning_rate and momentum_rate do not apply to this algorithm, so either of the first two configurations would be preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'sgd' - hidden layers and units\n",
    "Try a different number of hidden layers and units in the hidden layers with the 'sgd' algorithm and all other parameters fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "net_test_0, Y_class_test_0 = run(num_inputs,        [], class_list, X, T, X, 50000, 'sgd', 0.001, 0.0)\n",
    "net_test_1, Y_class_test_1 = run(num_inputs,      [50], class_list, X, T, X, 50000, 'sgd', 0.001, 0.0)\n",
    "net_test_2, Y_class_test_2 = run(num_inputs, [5, 5, 5], class_list, X, T, X, 50000, 'sgd', 0.001, 0.0)\n",
    "n_correct_0 = (Y_class_test_0 == T).sum()\n",
    "n_correct_1 = (Y_class_test_1 == T).sum()\n",
    "n_correct_2 = (Y_class_test_2 == T).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(net_test_0, net_test_1, net_test_2, T, Y_class_test_1, n_correct_0, n_correct_1, n_correct_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Networks with fewer hidden layers preform better than those with more hidden layers.  The best behavior is found with a single hidden layer, but it never fully trains even with a very large nuber of iterations.   With the best network, the network correctly predictss 100% of the datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'sgd' - single hidden layer, variable number of units\n",
    "Try a different number of units in a single hidden layer and all other parameters fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "net_test_0, Y_class_test_0 = run(num_inputs,  [1], class_list, X, T, X, 50000, 'sgd', 0.001, 0.0)\n",
    "net_test_1, Y_class_test_1 = run(num_inputs, [10], class_list, X, T, X, 50000, 'sgd', 0.001, 0.0)\n",
    "net_test_2, Y_class_test_2 = run(num_inputs, [50], class_list, X, T, X, 50000, 'sgd', 0.001, 0.0)\n",
    "n_correct_0 = (Y_class_test_0 == T).sum()\n",
    "n_correct_1 = (Y_class_test_1 == T).sum()\n",
    "n_correct_2 = (Y_class_test_2 == T).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "summary(net_test_0, net_test_1, net_test_2, T, Y_class_test_1, n_correct_0, n_correct_1, n_correct_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Networks with more units in the hidden laye preform better than those with fewer units.  The best network never fully trains even with a very large nuber of iterations. With the best network, the network correctly predictss 100% of the datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'sgd' - single hidden layer, variable learning rate\n",
    "Try a different learning rate with all other parameters fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "net_test_0, Y_class_test_0 = run(num_inputs, [50], class_list, X, T, X, 50000, 'sgd', 0.001, 0.0)\n",
    "net_test_1, Y_class_test_1 = run(num_inputs, [50], class_list, X, T, X, 50000, 'sgd', 0.01,  0.0)\n",
    "net_test_2, Y_class_test_2 = run(num_inputs, [50], class_list, X, T, X, 50000, 'sgd', 0.1,   0.0)\n",
    "n_correct_0 = (Y_class_test_0 == T).sum()\n",
    "n_correct_1 = (Y_class_test_1 == T).sum()\n",
    "n_correct_2 = (Y_class_test_2 == T).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "summary(net_test_0, net_test_1, net_test_2, T, Y_class_test_0, n_correct_0, n_correct_1, n_correct_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate has very little affet on the overall performance of the neural network.  All configurations never fully train even with a very large number of iterations. With the best network, the network correctly predictss 100% of the datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'sgd' - single hidden layer, variable momentum rate\n",
    "Try a different momentum rate with all other parameters fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "net_test_0, Y_class_test_0 = run(num_inputs, [50], class_list, X, T, X, 50000, 'sgd', 0.001, 0.0)\n",
    "net_test_1, Y_class_test_1 = run(num_inputs, [50], class_list, X, T, X, 50000, 'sgd', 0.001, 0.1)\n",
    "net_test_2, Y_class_test_2 = run(num_inputs, [50], class_list, X, T, X, 50000, 'sgd', 0.001, 0.5)\n",
    "n_correct_0 = (Y_class_test_0 == T).sum()\n",
    "n_correct_1 = (Y_class_test_1 == T).sum()\n",
    "n_correct_2 = (Y_class_test_2 == T).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(net_test_0, net_test_1, net_test_2, T, Y_class_test_0, n_correct_0, n_correct_1, n_correct_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The momentum rate has very little affet on the overall performance of the neural network. All configurations never fully train even with a very large number of iterations. With the best network, the network correctly predictss 100% of the datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'adam' - hidden layers and units\n",
    "Try a different number of hidden layers and units in the hidden layers with the 'adam' algorithm and all other parameters fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "net_test_0, Y_class_test_0 = run(num_inputs,        [], class_list, X, T, X, 10000, 'adam', 0.001, 0.0)\n",
    "net_test_1, Y_class_test_1 = run(num_inputs,      [50], class_list, X, T, X, 10000, 'adam', 0.001, 0.0)\n",
    "net_test_2, Y_class_test_2 = run(num_inputs, [5, 5, 5], class_list, X, T, X, 10000, 'adam', 0.001, 0.0)\n",
    "n_correct_0 = (Y_class_test_0 == T).sum()\n",
    "n_correct_1 = (Y_class_test_1 == T).sum()\n",
    "n_correct_2 = (Y_class_test_2 == T).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(net_test_0, net_test_1, net_test_2, T, Y_class_test_2, n_correct_0, n_correct_1, n_correct_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network with a single hidden layer performs better than those with more or less, but all configurations train after about 8000 iterations. Once trained, the network correctly predictss 100% of the datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'adam' - single hidden layer, variable momentum rate\n",
    "Try a different momentum rate with all other parameters fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "net_test_0, Y_class_test_0 = run(num_inputs,   [1], class_list, X, T, X, 10000, 'adam', 0.001, 0.0)\n",
    "net_test_1, Y_class_test_1 = run(num_inputs,  [10], class_list, X, T, X, 10000, 'adam', 0.001, 0.0)\n",
    "net_test_2, Y_class_test_2 = run(num_inputs, [100], class_list, X, T, X, 10000, 'adam', 0.001, 0.0)\n",
    "n_correct_0 = (Y_class_test_0 == T).sum()\n",
    "n_correct_1 = (Y_class_test_1 == T).sum()\n",
    "n_correct_2 = (Y_class_test_2 == T).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "summary(net_test_0, net_test_1, net_test_2, T, Y_class_test_2, n_correct_0, n_correct_1, n_correct_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network with the most units in the hidden layer performs better than those with less.  The single hidden layer significantly reducces the number of iterations needed to train the network; the network with no hidden layers doesn't train in 10000 cycles while those with a hidden layer train in 2000 or less.  Once trained, the best network correctly predictss 100% of the datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'adam' - single hidden layer, variable learning rate\n",
    "Try a different learning rate with all other parameters fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "net_test_0, Y_class_test_0 = run(num_inputs, [100], class_list, X, T, X, 1000, 'adam', 0.001, 0.0)\n",
    "net_test_1, Y_class_test_1 = run(num_inputs, [100], class_list, X, T, X, 1000, 'adam', 0.01,  0.0)\n",
    "net_test_2, Y_class_test_2 = run(num_inputs, [100], class_list, X, T, X, 1000, 'adam', 0.1,   0.0)\n",
    "n_correct_0 = (Y_class_test_0 == T).sum()\n",
    "n_correct_1 = (Y_class_test_1 == T).sum()\n",
    "n_correct_2 = (Y_class_test_2 == T).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(net_test_0, net_test_1, net_test_2, T, Y_class_test_2, n_correct_0, n_correct_1, n_correct_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate has very little affet on the overall performance of the neural network. All configurations fully train around 600 iterations. All trained networks correctly predict 100% of the datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'adam' - single hidden layer, variable momentum rate\n",
    "Try a momentum learning rate with all other parameters fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "net_test_0, Y_class_test_0 = run(num_inputs, [100], class_list, X, T, X, 1000, 'adam', 0.001, 0.0)\n",
    "net_test_1, Y_class_test_1 = run(num_inputs, [100], class_list, X, T, X, 1000, 'adam', 0.001, 0.1)\n",
    "net_test_2, Y_class_test_2 = run(num_inputs, [100], class_list, X, T, X, 1000, 'adam', 0.001, 0.5)\n",
    "n_correct_0 = (Y_class_test_0 == T).sum()\n",
    "n_correct_1 = (Y_class_test_1 == T).sum()\n",
    "n_correct_2 = (Y_class_test_2 == T).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(net_test_0, net_test_1, net_test_2, T, Y_class_test_2, n_correct_0, n_correct_1, n_correct_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The momentum rate has very little affet on the overall performance of the neural network. All configurations fully train around 600 iterations. All trained networks correctly predict 100% of the datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was able to find several network configurations and training algorithms that yield good overall training results for the 'wine' data.  The 'scg' algorithm works best for training since all configurations tested fully train around 15 iterations. The 'adam' algorithm works fairly well since most configurations do fully train (1.000), but requires ~600 iterations to do so.  The 'sgd' algorithm has the worst training performance since no configuration results in full training despite the largest amount of iterations. In all cases, I did not see any statistical difference based on the learning_rate or momentum_rate.\n",
    "\n",
    "Once the networks were trained, I was able to find several configurations that do a very good job at predicting the correct class.  This is somewhat expected given the fact that we are using all of the training data to test the network.  I am somewhat surprised that the network preformed so well given the limited number samples for each class. It will be interesting to see the Extra Credit results (below) to see how the network behaves with less training data and on test data it has not yet seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grading\n",
    "\n",
    "Download [A3grader.zip](https://www.cs.colostate.edu/~anderson/cs545/notebooks/A3grader.zip) and extract A3grader.py from it. Run the code in the following cell to demonstrate an example grading session. You should see a perfect execution score of 80/80 if your functions are defined correctly. Other tests will be performed on your checked in notebook to make up the 80 execution points. The remaining 20 points will be based on the results you obtain from the classification data you download and your discussion of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i A3grader.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For the data set you have downloaded, randomly partition the data into about 80% for training and 20% for testing.  Discuss the percent of samples correctly classified for train and test partitions and how variations in netowrk size, epochs, and optimization algorithm effect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload data from the CSV file\n",
    "data = np.loadtxt('wine.data', delimiter=\",\")\n",
    "\n",
    "# NOTE: The data is ordered based on the resulting class.  Since we are using a subset for training\n",
    "#       and test, we need an random distrubution of the data.  The simplest way to get this is to \n",
    "#       randomly shuffle the rows, which is what I'm doing in the next step\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# The first column is the class - Take all rows, first column\n",
    "T_shfl = data[:,0].reshape(-1,1).astype(int)\n",
    "\n",
    "# Get class list from the Target Data\n",
    "class_list_shfl = list(np.unique(T_shfl))\n",
    "\n",
    "# Data is everything except first column - Take all rows, all but first colum\n",
    "X_shfl = data[:,1:]\n",
    "\n",
    "# Get the number of inputs from the data\n",
    "num_inputs = X_shfl.shape[1]\n",
    "\n",
    "data_len = X_shfl.shape[0]\n",
    "num_train = int(data_len * 0.8)\n",
    "num_test = data_len - num_train\n",
    "X_train = X_shfl[0:num_train, :]\n",
    "T_train = T_shfl[0:num_train, :]\n",
    "X_test  = X_shfl[num_train:, :]\n",
    "T_test = T_shfl[num_train:, :]\n",
    "\n",
    "# Print some usefull info\n",
    "print(f'Total number of samples ({data_len}) num_train({num_train}) num_test({num_test})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'scg'\n",
    "Try a different number of hidden layers and units in the hidden layers with the 'scg' algorithm and all other parameters fixed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "net_test_0, Y_class_test_0 = run(num_inputs,          [],  class_list, X_train, T_train, X_test, 500, 'scg', 0.001, 0.0)\n",
    "net_test_1, Y_class_test_1 = run(num_inputs,        [50],  class_list, X_train, T_train, X_test, 500, 'scg', 0.001, 0.0)\n",
    "net_test_2, Y_class_test_2 = run(num_inputs, [50, 20, 20], class_list, X_train, T_train, X_test, 500, 'scg', 0.001, 0.0)\n",
    "n_correct_0 = (Y_class_test_0 == T_test).sum()\n",
    "n_correct_1 = (Y_class_test_1 == T_test).sum()\n",
    "n_correct_2 = (Y_class_test_2 == T_test).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "summary(net_test_0, net_test_1, net_test_2, T_test, Y_class_test_2, n_correct_0, n_correct_1, n_correct_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network behaves the same as the previous experiment using all data for training.  It performs better with fewer hidden layers and all train within 20 iterations.  Using all data, the network trained in 15 iterations, so more data samples reduces the amount of time to train.  Once trained, the network correctly predictss 100% of the datapoints. The learning_rate and momentum_rate do not apply to this algorithm, so either of the first two configurations would be preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'sgd'\n",
    "Try a different number of hidden layers and units the hidden layers with the 'sgd' algorithm and all other parameters fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "net_test_0, Y_class_test_0 = run(num_inputs,        [], class_list, X_train, T_train, X_test, 50000, 'sgd', 0.001, 0.0)\n",
    "net_test_1, Y_class_test_1 = run(num_inputs,      [50], class_list, X_train, T_train, X_test, 50000, 'sgd', 0.001, 0.0)\n",
    "net_test_2, Y_class_test_2 = run(num_inputs, [5, 5, 5], class_list, X_train, T_train, X_test, 50000, 'sgd', 0.001, 0.0)\n",
    "n_correct_0 = (Y_class_test_0 == T_test).sum()\n",
    "n_correct_1 = (Y_class_test_1 == T_test).sum()\n",
    "n_correct_2 = (Y_class_test_2 == T_test).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(net_test_0, net_test_1, net_test_2, T_test, Y_class_test_2, n_correct_0, n_correct_1, n_correct_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network behaves the same as the previous experiment using all data for training.  Networks with fewer hidden layers preform better than those with more hidden layers. The best behavior is found with a single hidden layer, but it never fully trains even with a very large nuber of iterations. With the best network, the network correctly predictss 100% of the datapoints.  Overall, less data does not seem to have a noticable impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'sgd' - single hidden layer, variable number of units\n",
    "Try a different number of units in a single hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "net_test_0, Y_class_test_0 = run(num_inputs,  [1], class_list, X_train, T_train, X_test, 50000, 'sgd', 0.001, 0.0)\n",
    "net_test_1, Y_class_test_1 = run(num_inputs, [10], class_list, X_train, T_train, X_test, 50000, 'sgd', 0.001, 0.0)\n",
    "net_test_2, Y_class_test_2 = run(num_inputs, [50], class_list, X_train, T_train, X_test, 50000, 'sgd', 0.001, 0.0)\n",
    "n_correct_0 = (Y_class_test_0 == T_test).sum()\n",
    "n_correct_1 = (Y_class_test_1 == T_test).sum()\n",
    "n_correct_2 = (Y_class_test_2 == T_test).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(net_test_0, net_test_1, net_test_2, T_test, Y_class_test_2, n_correct_0, n_correct_1, n_correct_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network behaves the same as the previous experiment using all data for training.  Networks with more units in the hidden laye preform better than those with fewer units. The best network never fully trains even with a very large nuber of iterations. With the best network, the network correctly predictss 100% of the datapoints.   Overall, less data does not seem to have a noticable impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'sgd' - single hidden layer, variable learning rate\n",
    "Try a different learning rate with all other parameters fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "net_test_0, Y_class_test_0 = run(num_inputs, [50], class_list, X_train, T_train, X_test, 50000, 'sgd', 0.001, 0.0)\n",
    "net_test_1, Y_class_test_1 = run(num_inputs, [50], class_list, X_train, T_train, X_test, 50000, 'sgd', 0.01,  0.0)\n",
    "net_test_2, Y_class_test_2 = run(num_inputs, [50], class_list, X_train, T_train, X_test, 50000, 'sgd', 0.1,   0.0)\n",
    "n_correct_0 = (Y_class_test_0 == T_test).sum()\n",
    "n_correct_1 = (Y_class_test_1 == T_test).sum()\n",
    "n_correct_2 = (Y_class_test_2 == T_test).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(net_test_0, net_test_1, net_test_2, T_test, Y_class_test_2, n_correct_0, n_correct_1, n_correct_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network behaves the same as the previous experiment using all data for training.  The learning rate has very little affet on the overall performance of the neural network. All configurations never fully train even with a very large nuber of iterations. With the best network, the network correctly predictss 100% of the datapoints.  Overall, less data does not seem to have a noticable impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'sgd' - single hidden layer, variable momentum rate\n",
    "Try a different momentum rate with all other parameters fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "net_test_0, Y_class_test_0 = run(num_inputs,        [], class_list, X_train, T_train, X_test, 50000, 'sgd', 0.001, 0.0)\n",
    "net_test_1, Y_class_test_1 = run(num_inputs,      [50], class_list, X_train, T_train, X_test, 50000, 'sgd', 0.001, 0.1)\n",
    "net_test_2, Y_class_test_2 = run(num_inputs, [5, 5, 5], class_list, X_train, T_train, X_test, 50000, 'sgd', 0.001, 0.5)\n",
    "n_correct_0 = (Y_class_test_0 == T_test).sum()\n",
    "n_correct_1 = (Y_class_test_1 == T_test).sum()\n",
    "n_correct_2 = (Y_class_test_2 == T_test).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(net_test_0, net_test_1, net_test_2, T_test, Y_class_test_2, n_correct_0, n_correct_1, n_correct_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network behave simlarly to the previous experiment using all data for training.  One observation that is different is that the training does better for a single hidden layer comparted to the other configurations.  This is contrast to the the original experiment where all networks had nearly identical curves.  The training for all are above 0.95 but are a little more spread than than before.  Even with this difference, the best network correctly predictss 100% of the datapoints. Overall, less data does seem to have a noticable impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'adam' - hidden layers and units\n",
    "Try a different number of hidden layers and units the hidden layers with the 'adam' algorithm and all other parameters fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "net_test_0, Y_class_test_0 = run(num_inputs,        [], class_list, X_train, T_train, X_test, 10000, 'adam', 0.001, 0.0)\n",
    "net_test_1, Y_class_test_1 = run(num_inputs,      [50], class_list, X_train, T_train, X_test, 10000, 'adam', 0.001, 0.0)\n",
    "net_test_2, Y_class_test_2 = run(num_inputs, [5, 5, 5], class_list, X_train, T_train, X_test, 10000, 'adam', 0.001, 0.0)\n",
    "n_correct_0 = (Y_class_test_0 == T_test).sum()\n",
    "n_correct_1 = (Y_class_test_1 == T_test).sum()\n",
    "n_correct_2 = (Y_class_test_2 == T_test).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(net_test_0, net_test_1, net_test_2, T_test, Y_class_test_2, n_correct_0, n_correct_1, n_correct_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network behaves the same as the previous experiment using all data for training.  The network with a single hidden layer performs better than those with more or less, but all configurations train after about 8000 iterations. Once trained, the network correctly predictss 100% of the datapoints.  Overall, less data does not seem to have a noticable impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'adam' - single hidden layer, variable momentum rate\n",
    "Try a different momentum rate with all other parameters fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "net_test_0, Y_class_test_0 = run(num_inputs,   [1], class_list, X_train, T_train, X_test, 10000, 'adam', 0.001, 0.0)\n",
    "net_test_1, Y_class_test_1 = run(num_inputs,  [10], class_list, X_train, T_train, X_test, 10000, 'adam', 0.001, 0.0)\n",
    "net_test_2, Y_class_test_2 = run(num_inputs, [100], class_list, X_train, T_train, X_test, 10000, 'adam', 0.001, 0.0)\n",
    "n_correct_0 = (Y_class_test_0 == T_test).sum()\n",
    "n_correct_1 = (Y_class_test_1 == T_test).sum()\n",
    "n_correct_2 = (Y_class_test_2 == T_test).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(net_test_0, net_test_1, net_test_2, T_test, Y_class_test_2, n_correct_0, n_correct_1, n_correct_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network behaves the same as the previous experiment using all data for training.  The network with the most units in the hidden layer performs better than those with less. The increased units significantly reducces the number of iterations needed to train the network (~8000 to ~500). Once trained, the network correctly predictss 100% of the datapoints.  Overall, less data does not seem to have a noticable impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'adam' - single hidden layer, variable learning rate\n",
    "Try a different learning rate with all other parameters fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "net_test_0, Y_class_test_0 = run(num_inputs, [100], class_list, X_train, T_train, X_test, 1000, 'adam', 0.001, 0.0)\n",
    "net_test_1, Y_class_test_1 = run(num_inputs, [100], class_list, X_train, T_train, X_test, 1000, 'adam', 0.01,  0.0)\n",
    "net_test_2, Y_class_test_2 = run(num_inputs, [100], class_list, X_train, T_train, X_test, 1000, 'adam', 0.1,   0.0)\n",
    "n_correct_0 = (Y_class_test_0 == T_test).sum()\n",
    "n_correct_1 = (Y_class_test_1 == T_test).sum()\n",
    "n_correct_2 = (Y_class_test_2 == T_test).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(net_test_0, net_test_1, net_test_2, T_test, Y_class_test_2, n_correct_0, n_correct_1, n_correct_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network behaves the same as the previous experiment using all data for training.  The learning rate has very little affet on the overall performance of the neural network. All configurations fully train around 600 iterations. All trained networks correctly predict 100% of the datapoints.  Overall, less data does not seem to have a noticable impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'adam' - single hidden layer, variable momentum rate\n",
    "Try a momentum learning rate with all other parameters fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "net_test_0, Y_class_test_0 = run(num_inputs, [100], class_list, X_train, T_train, X_test, 1000, 'adam', 0.001, 0.0)\n",
    "net_test_1, Y_class_test_1 = run(num_inputs, [100], class_list, X_train, T_train, X_test, 1000, 'adam', 0.001, 0.1)\n",
    "net_test_2, Y_class_test_2 = run(num_inputs, [100], class_list, X_train, T_train, X_test, 1000, 'adam', 0.001, 0.5)\n",
    "n_correct_0 = (Y_class_test_0 == T_test).sum()\n",
    "n_correct_1 = (Y_class_test_1 == T_test).sum()\n",
    "n_correct_2 = (Y_class_test_2 == T_test).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(net_test_0, net_test_1, net_test_2, T_test, Y_class_test_2, n_correct_0, n_correct_1, n_correct_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network behaves the same as the previous experiment using all data for training.  The momentum rate has very little affet on the overall performance of the neural network. All configurations fully train around 600 iterations. All trained networks correctly predict 100% of the datapoints.  Overall, less data does not seem to have a noticable impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was able to find several network configurations and training algorithms that yield good overall results for the reduced data experiment. The results were very similar to the original experiment (more training data) where 'scg' performed best, followed by 'adam' then 'sgd'.  The one difference found was that the 'sgd' algorithm seemed to show more variance with less data, but even then, I still found a network with 100% accuracy.  My biggest take away is that the thirteen data points for each entry must have contained enough information to compensate for the reduced amount of data.  It's not yet clear to me which of the thirteen factors are most important, but a potential future experiment would be to manipulate the data and see which factors/combination of factors are most important."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
