{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3.2 Classification\n",
    "\n",
    "* 3.2: Added the code for `_objective_to_actual` and more calls to set the random number seed each place it is needed to help you test your code. Also made small change in `neuralnetworks.py`, so please download `A3code.zip` again and unzip it.\n",
    "* 3.1: Replaced example output from `python neuralnetworks.py` with correct values.  Added call to `np.random.seed` to set the seed value for the random number generator so your example runs will produce same output as the output included here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brent Staab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will define a new class named `NeuralNetworkClassifier` that extends the given class `NeuralNetwork`.  Your new class will reuse most of the code in its parent class, but you will have to define the parts of the code that differ to allow the network to do classification. \n",
    "\n",
    "To learn a bit about how to call methods in a parent class, take a look at this\n",
    "[introduction to inheritance](https://realpython.com/python-super/) and examples of using `super()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by downloading [A3code.zip](https://www.cs.colostate.edu/~anderson/cs545/notebooks/A3code.zip).  Unzip this and you should see these files:\n",
    "\n",
    "*  `mlutilities.py`\n",
    "*  `neuralnetworks.py`\n",
    "*  `optimizers.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just run `neuralnetworks.py` you should see the following result.\n",
    "\n",
    "```\n",
    "$ python neuralnetworks.py \n",
    "scg  [] use_torch=False RMSE 7.266 took 0.001 seconds\n",
    "scg  [5, 5] use_torch=False RMSE 1.561 took 0.088 seconds\n",
    "sgd  [5, 5] use_torch=False RMSE 0.856 took 0.040 seconds\n",
    "adam [5, 5] use_torch=False RMSE 0.247 took 0.044 seconds\n",
    "scg  [] use_torch=True RMSE 7.266 took 2.290 seconds\n",
    "scg  [5, 5] use_torch=True RMSE 0.213 took 4.553 seconds\n",
    "sgd  [5, 5] use_torch=True RMSE 0.803 took 1.420 seconds\n",
    "adam [5, 5] use_torch=True RMSE 0.261 took 1.434 seconds\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Called np.random.seed(42)\n",
      "scg  [] use_torch=False RMSE 7.266 took 0.001 seconds\n",
      "scg  [5, 5] use_torch=False RMSE 0.809 took 0.092 seconds\n",
      "sgd  [5, 5] use_torch=False RMSE 0.635 took 0.043 seconds\n",
      "adam [5, 5] use_torch=False RMSE 0.196 took 0.044 seconds\n",
      "scg  [] use_torch=True RMSE 7.266 took 2.110 seconds\n",
      "scg  [5, 5] use_torch=True RMSE 0.220 took 5.507 seconds\n",
      "sgd  [5, 5] use_torch=True RMSE 0.757 took 1.804 seconds\n",
      "adam [5, 5] use_torch=True RMSE 0.240 took 1.812 seconds\n"
     ]
    }
   ],
   "source": [
    "!python neuralnetworks.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changes Required\n",
    "\n",
    "The first line of your new class definition will be\n",
    "\n",
    "```\n",
    "   .\n",
    "   .\n",
    "   .\n",
    "import neuralnetworks as nn\n",
    "\n",
    "class NeuralNetworkClassifier(nn.NeuralNetwork):\n",
    "   .\n",
    "   .\n",
    "   .```\n",
    "\n",
    "Only one change is required in the arguments for all functions, and the change is in the constructor arguments.  The constructor must be dchanged from how it is defined in the `NeuralNetwork` class\n",
    "```\n",
    "    def __init__(self, n_inputs, n_hiddens_list, n_outputs, use_torch=False):\n",
    "```\n",
    "to\n",
    "```\n",
    "    def __init__(self, n_inputs, n_hiddens_list, classes, use_torch=False):\n",
    "```\n",
    "where ```classes``` is just a list of class integer names, like `[1, 2, 3]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only methods that you will have to include in your new `NeuralNetworkClassifier` are\n",
    "```\n",
    "__init__\n",
    "__repr__\n",
    "_standardizeT\n",
    "_unstandardizeT\n",
    "_forward_pass\n",
    "_objectiveF\n",
    "_objective_to_actual\n",
    "train\n",
    "use\n",
    "```\n",
    "\n",
    "You may introduce new methods that these use, such as\n",
    "```\n",
    "_make_indicator_variables```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of `NeuralNetworkClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a start at defining your new class.  This bit of code includes everything you will need in the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import mlutilities as ml  # for ml.draw\n",
    "import optimizers as opt  # for opt.sgd, opt.adam, and opt.scg\n",
    "\n",
    "import neuralnetworks as nn\n",
    "\n",
    "# Extend the nn.NeuralNetwork class to reuse much of its implementation\n",
    "# Only those methods that must be altered to do classification are defined in NeuralNetworkClassifier\n",
    "class NeuralNetworkClassifier(nn.NeuralNetwork):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, n_inputs, n_hiddens_list, classes, use_torch=False):\n",
    "\n",
    "        # Force n_hidens_list to be a list\n",
    "        if not isinstance(n_hiddens_list, list):\n",
    "            raise Exception('NeuralNetworkClassifier: n_hiddens_list must be a list.')\n",
    " \n",
    "        # Call the constructor for NeuralNetwork, passing in the number of unique class names (ints)\n",
    "        # as the number of outputs\n",
    "        super().__init__(n_inputs, n_hiddens_list, len(classes), use_torch)\n",
    "\n",
    "        # Store as member variables other things needed by instances of this class.\n",
    "        self.classes = np.array(classes) # to allow argmax in use()\n",
    "        \n",
    "        if use_torch:\n",
    "            self.log = torch.log\n",
    "            self.exp = torch.exp\n",
    "        else:\n",
    "            self.log = np.log\n",
    "            self.exp = np.exp\n",
    "    \n",
    "        self.n_samples = n_inputs\n",
    "        self.n_outputs = len(classes)\n",
    "\n",
    "    # insert the rest of your class methods here.\n",
    "    def __repr__(self):\n",
    "        str = f'{type(self).__name__}({self.n_inputs}, {self.n_hiddens_list}, {self.n_outputs}, use_torch={self.use_torch})'\n",
    "        if self.trained:\n",
    "            str += f'\\n   Network was trained for {self.n_epochs} epochs'\n",
    "            str += f' that took {self.training_time:.4f} seconds. Final objective value is {self.error_trace[-1]:.3f}'\n",
    "        else:\n",
    "            str += '  Network is not trained.'\n",
    "        return str\n",
    "    \n",
    "    def _standardizeT(self, T):\n",
    "        return T\n",
    "    \n",
    "    def _unstandardizeT(self, T):\n",
    "        return T\n",
    "    \n",
    "    def _forward_pass(self, X):\n",
    "        # BASE IMPLEMENTATION\n",
    "        #   # Assume weights already unpacked\n",
    "        #   Z_prev = X  # output of previous layer\n",
    "        #   Z = [Z_prev]\n",
    "        #   for i in range(self.n_hidden_layers):\n",
    "        #       V = self.Vs[i]\n",
    "        #       Z_prev = self.tanh(Z_prev @ V[1:, :] + V[0:1, :])\n",
    "        #       Z.append(Z_prev)\n",
    "        #   Y = Z_prev @ self.W[1:, :] + self.W[0:1, :]\n",
    "        #   return Y, Z\n",
    "        \n",
    "        \n",
    "        # Assume weights already unpacked\n",
    "        Z_prev = X  # output of previous layer\n",
    "        Z = [Z_prev]\n",
    "        for i in range(self.n_hidden_layers):\n",
    "            V = self.Vs[i]\n",
    "            Z_prev = self.tanh(Z_prev @ V[1:, :] + V[0:1, :])\n",
    "            Z.append(Z_prev)\n",
    "        Y = Z_prev @ self.W[1:, :] + self.W[0:1, :]\n",
    "        return Y, Z\n",
    "    \n",
    "        #return super()._forward_pass(X)\n",
    "    \n",
    "    def _objectiveF(self, w, X, T):\n",
    "        # Base implementation is:\n",
    "        #   self._unpack(w)\n",
    "        #   Y, _ = self._forward_pass(X)\n",
    "        #   return 0.5 * self.mean((T - Y)**2)\n",
    "        #\n",
    "        # bstaab - this is the function discussed in 'Use function'\n",
    "        #          change from MSE to negative mean log\n",
    "        #\n",
    "        return super()._objectiveF(w,X,T)\n",
    "    \n",
    "    def _objective_to_actual(self, neg_mean_log_likelihood):\n",
    "        return self.exp(- neg_mean_log_likelihood)\n",
    "    \n",
    "    def train(self, X, T, n_epochs, method='scg',\n",
    "              verbose=False, save_weights_history=False,\n",
    "              learning_rate=0.001, momentum_rate=0.0): # only for sgd and adam\n",
    "\n",
    "        if X.shape[1] != self.n_inputs:\n",
    "            raise Exception(f'train: number of columns in X ({X.shape[1]}) not equal to number of network inputs ({self.n_inputs})')\n",
    "        \n",
    "        if self.use_torch:\n",
    "            X = torch.tensor(X, dtype=torch.float)  # 32 bit\n",
    "            T = torch.tensor(T, dtype=torch.float)\n",
    "\n",
    "        self._setup_standardize(X, T)\n",
    "        X = self._standardizeX(X)\n",
    "        T = self._standardizeT(T)\n",
    "        \n",
    "        try:\n",
    "            algo = [opt.sgd, opt.adam, opt.scg][['sgd', 'adam', 'scg'].index(method)]\n",
    "        except:\n",
    "            raise Exception(\"train: method={method} not one of 'scg', 'sgd' or 'adam'\")            \n",
    "\n",
    "        result = algo(self._pack(self.Vs, self.W),\n",
    "                      self._objectiveF,                  # Need to fix this\n",
    "                      [X, T], \n",
    "                      n_epochs, \n",
    "                      self._gradientF,                   # USE BASE CLASS IMPLEMENTATION\n",
    "                      eval_f=self._objective_to_actual,  # GIVEN BY INSTRUCTOR\n",
    "                      learning_rate=learning_rate,      \n",
    "                      momentum_rate=momentum_rate,\n",
    "                      verbose=verbose, \n",
    "                      use_torch=self.use_torch,\n",
    "                      save_wtrace=save_weights_history)\n",
    "\n",
    "        self._unpack(result['w'])\n",
    "        self.reason = result['reason']\n",
    "        self.error_trace = result['ftrace'] # * self.Tstds # to _unstandardize the MSEs\n",
    "        self.n_epochs = len(self.error_trace) - 1\n",
    "        self.trained = True\n",
    "        self.weight_history = result['wtrace'] if save_weights_history else None\n",
    "        self.training_time = result['time']\n",
    "        return self\n",
    "    \n",
    "    def use(self, X, all_outputs=False):\n",
    "        return super().use(X, all_outputs)\n",
    "    \n",
    "    # Function to convert a list of target values into an array \n",
    "    # of 'indicator variables'\n",
    "    def _make_indicator_variables(self, T):\n",
    "        # Make sure T is two-dimensiona. Should be nSamples x 1.\n",
    "        if T.ndim == 1:\n",
    "            T = T.reshape((-1,1))    \n",
    "        return (T == np.unique(T)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Tests\n",
    "\n",
    "Here are some tests of your code and what the results should show.  First the original `NeuralNetwork` class methods are called then the new ones you define in `NeuralNetworkClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import neuralnetworks as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some toy data.  Input samples have two components, and all values are from 0 to 1.9.  Two targets are defined. The first is the square of of the first input component.  The second is the sine of the second component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(20).reshape((10, 2)) * 0.1\n",
    "T = np.hstack((X[:, 0:1]**2, np.sin(X[:, 1:2])))\n",
    "X.shape, T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(T)\n",
    "plt.xlabel('Sample Index')\n",
    "plt.legend(['Square', 'Sine']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)  # only to help you compare your output to mine.  Do not use otherwise.\n",
    "\n",
    "nnet = nn.NeuralNetwork(2, [10, 10], 2)\n",
    "nnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet.train(X, T, 50, method='scg')\n",
    "nnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(nnet.get_error_trace())\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('RMSE');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = nnet.use(X)\n",
    "plt.plot(T)\n",
    "plt.plot(Y, '--')\n",
    "plt.legend(['$T_0$', '$T_1$', '$Y_0$', '$Y_1']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)  # only to help you compare your output to mine.  Do not use otherwise.\n",
    "\n",
    "nnet = nn.NeuralNetwork(2, [10, 10], 2)\n",
    "nnet.train(X, T, 50, method='sgd', learning_rate=0.1, momentum_rate=0.5)\n",
    "print(nnet)\n",
    "plt.plot(nnet.get_error_trace())\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('RMSE');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = nnet.use(X)\n",
    "plt.plot(T)\n",
    "plt.plot(Y, '--')\n",
    "plt.legend(['$T_0$', '$T_1$', '$Y_0$', '$Y_1']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and with Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)  # only to help you compare your output to mine.  Do not use otherwise.\n",
    "\n",
    "nnet = nn.NeuralNetwork(2, [10, 10], 2)\n",
    "nnet.train(X, T, 50, method='adam', learning_rate=0.1)\n",
    "print(nnet)\n",
    "plt.plot(nnet.get_error_trace())\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('RMSE');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = nnet.use(X)\n",
    "plt.plot(T)\n",
    "plt.plot(Y, '--')\n",
    "plt.legend(['$T_0$', '$T_1$', '$Y_0$', '$Y_1']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the use of pytorch, add `use_torch=True` to the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)  # only to help you compare your output to mine.  Do not use otherwise.\n",
    "\n",
    "nnet = nn.NeuralNetwork(2, [10, 10], 2, use_torch=True)\n",
    "nnet.train(X, T, 50, method='adam', learning_rate=0.1)\n",
    "print(nnet)\n",
    "plt.plot(nnet.get_error_trace())\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('RMSE');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = nnet.use(X)\n",
    "plt.plot(T)\n",
    "plt.plot(Y, '--')\n",
    "plt.legend(['$T_0$', '$T_1$', '$Y_0$', '$Y_1']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for some tests on the individual new class methods.  First we create some toy classification data.  Each sample has two inputs, randomly chosen from integers 0, 1, and 2.  Target class is 1 if the two input samples are equal, 0 if they are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)  # only to help you compare your output to mine.  Do not use otherwise.\n",
    "\n",
    "n_samples = 20\n",
    "X = np.random.choice(3, (n_samples, 2))\n",
    "T = (X[:, 0:1] == X[:, 1:2]).astype(int)  # where the two inputs are equal\n",
    "classes = [0, 1]\n",
    "\n",
    "for x, t in zip(X, T):\n",
    "    print(f'x = {x}, t = {t}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{np.sum(T==0)} not equal, {np.sum(T==1)} equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bstaab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)  # only to help you compare your output to mine.  Do not use otherwise.\n",
    "\n",
    "nnet_new = NeuralNetworkClassifier(2, [10, 10], [0, 1])\n",
    "nnet_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not be standardizing targets T for this network, so we must redefine the two relevant functions so they just return their argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet_new._standardizeT(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet_new._unstandardizeT(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standardization functions from the parent class will be available to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet_new._setup_standardize(X, T)\n",
    "Xst = nnet_new._standardizeX(X)\n",
    "Xst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this test of `_forward_pass` your results will differ, because they depend on the randomly-initialized weight values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y, Z = nnet_new._forward_pass(Xst)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FORWARD PASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soft max\n",
    "def g(X,w):\n",
    "    fs = np.exp(X @ w)  # N x K\n",
    "    denom = np.sum(fs, axis=1).reshape((-1,1))\n",
    "    gs = fs / denom\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = nnet_new._pack(nnet_new.Vs, nnet_new.W)\n",
    "\n",
    "#T_indicator_vars = np.hstack((T, 1 - T))  # this only works for this particular two-class toy data\n",
    "T_indicator_vars = nnet_new._make_indicator_variables(T)\n",
    "\n",
    "nnet_new._objectiveF(w, X, T_indicator_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conversion from objective function, which is being minimized, to the likelihood of the data that we wish to maximize, is done by your new definition of `_objective_to_actual`.  This function is given as its argument the output from `_objectiveF` which is the negative mean log likelihood.  It should convert this to just likelihood. Do this by removing the operations in reverse order.  This is confusing a number of you, so I will give you the correct code here.\n",
    "\n",
    "\n",
    "    def _objective_to_actual(self, neg_mean_log_likelihood):\n",
    "        return self.exp(- neg_mean_log_likelihood * self.n_samples * self.n_outputs)\n",
    "        \n",
    "You should set `self.n_samples` in the `train` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the workhorses, the functions the user of your class will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nnet_new.train(X, T, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_classes, Y = nnet_new.use(X)\n",
    "Y_classes.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot target and predicted classes. Shift the predicted class a bit upward to we can see it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(nnet_new.get_error_trace())\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Likelihood')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(T, 'o-')\n",
    "plt.plot(Y_classes + 0.05, 'o-')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Class')\n",
    "plt.legend(['Target Class', 'Predicted Class']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Data Set\n",
    "\n",
    "Now download a dataset for a classification problem from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets.php).  Choose one you find interesting.\n",
    "\n",
    "Load the data into `numpy` arrays for `X` and `T`.  Use your `NeuralNetworkClassifier` to model the data.  Train on all of the data and calculate the number of samples that you correctly classify.  Compare the accuracy for several sizes of networks, numbers of epochs, and optimization algorithms.  Discuss what you find.  Are you able to model the data accurately?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set I'm using is https://archive.ics.uci.edu/ml/datasets/Wine\n",
    "NOTE: 1st attribute is class identifier (1-3)  \n",
    "      Number of instances  \n",
    "         class 1 59  \n",
    "\t     class 2 71  \n",
    "\t     class 3 48  \n",
    "The analysis determined the quantities of 13 constituents found in each of the three types of wines         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from the CSV file\n",
    "data = np.loadtxt('wine.data', delimiter=\",\")\n",
    "#np.random.shuffle(data)\n",
    "\n",
    "# The first column is the class - Take all rows, first column\n",
    "T = data[:,0].reshape(-1,1).astype(int)\n",
    "\n",
    "# Data is everything except first column - Take all rows, all but first colum\n",
    "X = data[:,1:]\n",
    "print(f'X.shape({X.shape}), T.shape({T.shape}))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class list from the Target Data\n",
    "class_list = list(np.unique(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet_wine = NeuralNetworkClassifier(13, [10, 10], class_list)\n",
    "nnet_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet_wine.train(X, T, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet_wine._setup_standardize(X, T)\n",
    "Xst = nnet_wine._standardizeX(X)\n",
    "#Xst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y, Z = nnet_wine._forward_pass(Xst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grading\n",
    "\n",
    "A3grader.py will be available soon.  Until it is, conduct your own tests of your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit\n",
    "\n",
    "1. For the data set you have downloaded, randomly partition the data into about 80% for training and 20% for testing.  Discuss the percent of samples correctly classified for train and test partitions and how variations in netowrk size, epochs, and optimization algorithm effect the results.\n",
    "2. Develop a new version of `optimizers.py`, `neuralnetworks.py` and your `NeuralNetworkClassifier` class that will run on a GPU using pytorch.  Add a keyword argument where neded called `use_gpu` that can be set to `True` to run on GPU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
