{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS545: Machine Learning\n",
    "## Fall 2019 - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brent Staab and Seth Hughes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import code needed in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sys import platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of `NeuralNetwork_Convolutional` (from A4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork_Convolutional():\n",
    "    # Initilization function (a.k.a. constructor) for object\n",
    "    def __init__(self, \n",
    "                 n_channels_in_image,         # (int) number of values per pixel \n",
    "                 image_size,                  # (int) number of rows in image, same as number of columns\n",
    "                 n_units_in_conv_layers,      # (list of ints) number of units in each convolutional layer\n",
    "                 kernels_size_and_stride,     # (list of lists) each list is [kernel_size, kernel_stride] for each convolutional layer\n",
    "                 n_units_in_fc_hidden_layers, # (list of ints) number of units in fully-connected layers\n",
    "                 classes,                     # (list of ints) labels for each class\n",
    "                 use_gpu=False):              # (boolean) flag indicating GPU use or not (default = no GPU)\n",
    "\n",
    "        if not isinstance(n_units_in_conv_layers, list):\n",
    "            raise Exception('n_units_in_conv_layers must be a list')\n",
    "\n",
    "        if not isinstance(n_units_in_fc_hidden_layers, list):\n",
    "            raise Exception('n_units_in_fc_hidden_layers must be a list')\n",
    "        \n",
    "        if use_gpu and not torch.cuda.is_available():\n",
    "            print('\\nGPU is not available. Running on CPU.\\n')\n",
    "            use_gpu = False\n",
    "\n",
    "        self.n_channels_in_image = n_channels_in_image\n",
    "        self.image_size = image_size \n",
    "        self.n_units_in_conv_layers = n_units_in_conv_layers\n",
    "        self.n_units_in_fc_hidden_layers = n_units_in_fc_hidden_layers\n",
    "        self.kernels_size_and_stride = kernels_size_and_stride\n",
    "        self.n_outputs = len(classes)\n",
    "        self.classes = np.array(classes)\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "        self.n_conv_layers = len(self.n_units_in_conv_layers)\n",
    "        self.n_fc_hidden_layers = len(self.n_units_in_fc_hidden_layers)\n",
    "      \n",
    "        # Build the net layers\n",
    "        self.nnet = torch.nn.Sequential()\n",
    "\n",
    "        # Add convolutional layers\n",
    "        n_units_previous = self.n_channels_in_image\n",
    "        output_size_previous = self.image_size\n",
    "        n_layers = 0\n",
    "        if self.n_conv_layers > 0:\n",
    "\n",
    "            for (n_units, kernel) in zip(self.n_units_in_conv_layers, self.kernels_size_and_stride):\n",
    "                n_units_previous, output_size_previous = self._add_conv2d_tanh(n_layers,\n",
    "                                        n_units_previous, output_size_previous, n_units, kernel)\n",
    "                n_layers += 1 # for text label in layer\n",
    "                \n",
    "        self.nnet.add_module('flatten', torch.nn.Flatten())  # prepare for fc layers\n",
    "\n",
    "        n_inputs = output_size_previous ** 2 * n_units_previous\n",
    "        if self.n_fc_hidden_layers > 0:\n",
    "            for n_units in self.n_units_in_fc_hidden_layers:\n",
    "                n_inputs = self._add_fc_tanh(n_layers, n_inputs, n_units)\n",
    "                n_layers += 1\n",
    "\n",
    "        self.nnet.add_module(f'output_{n_layers}', torch.nn.Linear(n_inputs, self.n_outputs))\n",
    "        \n",
    "        # Define loss and optimizer functions\n",
    "        self.loss_F = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Member variables for standardization\n",
    "        self.Xmeans = None\n",
    "        self.Xstds = None\n",
    "\n",
    "        if self.use_gpu:\n",
    "            self.nnet.cuda()\n",
    "\n",
    "        self.n_epochs = 0\n",
    "        self.error_trace = []\n",
    "\n",
    "    def _add_conv2d_tanh(self, n_layers, n_units_previous, output_size_previous,\n",
    "                   n_units, kernel_size_and_stride):\n",
    "        kernel_size, kernel_stride = kernel_size_and_stride\n",
    "        self.nnet.add_module(f'conv_{n_layers}', torch.nn.Conv2d(n_units_previous, n_units,\n",
    "                                                                 kernel_size, kernel_stride))\n",
    "        self.nnet.add_module(f'output_{n_layers}', torch.nn.Tanh())\n",
    "        output_size_previous = (output_size_previous - kernel_size) // kernel_stride + 1\n",
    "        n_units_previous = n_units                \n",
    "        return n_units_previous, output_size_previous\n",
    "    \n",
    "    def _add_fc_tanh(self, n_layers, n_inputs, n_units):\n",
    "        self.nnet.add_module(f'linear_{n_layers}', torch.nn.Linear(n_inputs, n_units))\n",
    "        self.nnet.add_module(f'output_{n_layers}', torch.nn.Tanh())\n",
    "        n_inputs = n_units\n",
    "        return n_inputs\n",
    "\n",
    "    def __repr__(self):\n",
    "        str = f'''{type(self).__name__}(\n",
    "                            n_channels_in_image={self.n_channels_in_image},\n",
    "                            image_size={self.image_size},\n",
    "                            n_units_in_conv_layers={self.n_units_in_conv_layers},\n",
    "                            kernels_size_and_stride={self.kernels_size_and_stride},\n",
    "                            n_units_in_fc_hidden_layers={self.n_units_in_fc_hidden_layers},\n",
    "                            classes={self.classes},\n",
    "                            use_gpu={self.use_gpu})'''\n",
    "\n",
    "        str += self.nnet\n",
    "        if self.n_epochs > 0:\n",
    "            str += f'\\n   Network was trained for {self.n_epochs} epochs that took {self.training_time:.4f} seconds.'\n",
    "            str += f'\\n   Final objective value is {self.error_trace[-1]:.3f}'\n",
    "        else:\n",
    "            str += '  Network is not trained.'\n",
    "        return str\n",
    "        \n",
    "    def _standardizeX(self, X):\n",
    "        result = (X - self.Xmeans) / self.XstdsFixed\n",
    "        result[:, self.Xconstant] = 0.0\n",
    "        return result\n",
    "\n",
    "    def _unstandardizeX(self, Xs):\n",
    "        return self.Xstds * Xs + self.Xmeans\n",
    "\n",
    "    def _setup_standardize(self, X, T):\n",
    "        if self.Xmeans is None:\n",
    "            self.Xmeans = X.mean(axis=0)\n",
    "            self.Xstds = X.std(axis=0)\n",
    "            self.Xconstant = self.Xstds == 0\n",
    "            self.XstdsFixed = copy.copy(self.Xstds)\n",
    "            self.XstdsFixed[self.Xconstant] = 1\n",
    "\n",
    "    def train(self, X, T, n_epochs, learning_rate=0.01):\n",
    "\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        if T.ndim == 1:\n",
    "            T = T.reshape((-1, 1))\n",
    "\n",
    "        _, T = np.where(T == self.classes)  # convert to labels from 0\n",
    "\n",
    "        self._setup_standardize(X, T)\n",
    "        X = self._standardizeX(X)\n",
    "\n",
    "        X = torch.tensor(X)\n",
    "        T = torch.tensor(T.reshape(-1))\n",
    "        if self.use_gpu:\n",
    "            X = X.cuda()\n",
    "            T = T.cuda()\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.nnet.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        # You fill in the rest of the train function, following lecture notes example.\n",
    "        for epoch in range(n_epochs):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            Y = self.nnet(X)\n",
    "\n",
    "            error = self.loss_F(Y, T)\n",
    "            self.error_trace.append(error)\n",
    "            if epoch % 5 == 0:\n",
    "                print(f'Epoch {epoch} error {error:.5f}')\n",
    "\n",
    "            error.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "        \n",
    "        self.training_time = time.time() - start_time\n",
    "        \n",
    "    def get_error_trace(self):\n",
    "        return self.error_trace\n",
    "    \n",
    "    def _softmax(self, Y):\n",
    "        mx = Y.max()\n",
    "        expY = np.exp(Y - mx)\n",
    "        denom = expY.sum(axis=1).reshape((-1, 1)) + sys.float_info.epsilon\n",
    "        return expY / denom\n",
    "    \n",
    "    def use(self, X):\n",
    "        self.nnet.eval()  # turn off gradients and other aspects of training\n",
    "        X = self._standardizeX(X)\n",
    "        X = torch.tensor(X)\n",
    "        if self.use_gpu:\n",
    "            X = X.cuda()\n",
    "\n",
    "        Y = self.nnet(X)\n",
    "\n",
    "        if self.use_gpu:\n",
    "            Y = Y.cpu()\n",
    "        Y = Y.detach().numpy()\n",
    "        Yclasses = self.classes[Y.argmax(axis=1)].reshape((-1, 1))\n",
    "\n",
    "        return Yclasses, self._softmax(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm not sure why this doesn't work.  When I run the following two commands, I get the\n",
    "# output at the bottom of this cell.  I had to manually download from the browser and\n",
    "# unzip for this notebook to work.\n",
    "\n",
    "#!curl -O https://github.com/chadwickbureau/baseballdatabank/archive/v2019.2.zip\n",
    "#!unzip -o v2019.2.zip\n",
    "\n",
    "#---------------------------------------------------------------------------------\n",
    "#   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
    "#                                  Dload  Upload   Total   Spent    Left  Speed\n",
    "# 100   137    0   137    0     0    193      0 --:--:-- --:--:-- --:--:--   193\n",
    "# Archive:  v2019.2.zip\n",
    "#   End-of-central-directory signature not found.  Either this file is not\n",
    "#   a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
    "#   latter case the central directory and zipfile comment will be found on\n",
    "#   the last disk(s) of this archive.\n",
    "# unzip:  cannot find zipfile directory in one of v2019.2.zip or\n",
    "#         v2019.2.zip.zip, and cannot find v2019.2.zip.ZIP, period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform == 'linux':\n",
    "    people_file = 'baseballdatabank-2019.2/baseballdatabank-2019.2/core/People.csv'\n",
    "    batting_file = 'baseballdatabank-2019.2/baseballdatabank-2019.2/core/Batting.csv'\n",
    "    fielding_file = 'baseballdatabank-2019.2/baseballdatabank-2019.2/core/Fielding.csv'\n",
    "    pitchcing_file = 'baseballdatabank-2019.2/baseballdatabank-2019.2/core/Pitching.csv'\n",
    "    halloffame_file = 'baseballdatabank-2019.2/baseballdatabank-2019.2/core/HallOfFame.csv'\n",
    "    allstarfull_filie = 'baseballdatabank-2019.2/baseballdatabank-2019.2/core/AllstarFull.csv'\n",
    "else:\n",
    "    print(\"windows\")\n",
    "\n",
    "    people_file = 'C:\\\\Users\\\\user\\\\Notebooks\\\\CS545\\\\Project\\\\baseballdatabank-2019.2\\\\core\\\\People.csv'\n",
    "    batting_file = 'C:\\\\Users\\\\user\\\\Notebooks\\\\CS545\\\\Project\\\\baseballdatabank-2019.2\\\\core\\\\Batting.csv'\n",
    "    fielding_file = 'C:\\\\Users\\\\user\\\\Notebooks\\\\CS545\\\\Project\\\\baseballdatabank-2019.2\\\\core\\\\Fielding.csv'\n",
    "    pitchcing_file = 'C:\\\\Users\\\\user\\\\Notebooks\\\\CS545\\\\Project\\\\baseballdatabank-2019.2\\\\core\\\\Pitching.csv'\n",
    "    halloffame_file = 'C:\\\\Users\\\\user\\\\Notebooks\\\\CS545\\\\Project\\\\baseballdatabank-2019.2\\\\core\\\\HallofFame.csv'\n",
    "    allstarfull_filie = 'C:\\\\Users\\\\user\\\\Notebooks\\\\CS545\\\\Project\\\\baseballdatabank-2019.2\\\\core\\\\AllstarFull.csv'\n",
    "\n",
    "# Players\n",
    "df_players = pandas.read_csv(people_file)\n",
    "df_players = df_players['playerID']\n",
    "\n",
    "# Batting information for players\n",
    "df_batting = pandas.read_csv(batting_file)\n",
    "df_batting.fillna(0, inplace=True)\n",
    "\n",
    "# Fielding information\n",
    "df_fielding = pandas.read_csv(fielding_file)\n",
    "df_fielding.fillna(0, inplace=True)\n",
    "\n",
    "# Pitching information\n",
    "df_pitching = pandas.read_csv(pitchcing_file)\n",
    "df_pitching.fillna(0, inplace=True)\n",
    "\n",
    "# HoF\n",
    "df_hof = pandas.read_csv(halloffame_file)\n",
    "df_hof.fillna(0, inplace=True)\n",
    "df_hof = df_hof.loc[(df_hof['inducted'] == 'Y') & (df_hof['category'] == 'Player')]\n",
    "df_hof = df_hof['playerID']\n",
    "hof_list = list(df_hof.unique())\n",
    "\n",
    "#All-Star\n",
    "df_allstar = pandas.read_csv(allstarfull_filie)\n",
    "df_allstar.fillna(0, inplace=True)\n",
    "# allstar_list = list(df_allstar.unique()) # list of all star players\n",
    "df_allstar = df_allstar['playerID']\n",
    "allstar_list = list(df_allstar.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate career statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Pitching Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe to hold career stats for pitchers\n",
    "# A pitcher is anyone showing up in the 'df_pitching' dataframe, which was imported above\n",
    "df_career_pitching = pandas.DataFrame()\n",
    "\n",
    "if os.path.isfile('df_career_pitching.csv'):\n",
    "    df_career_pitching = pandas.read_csv('df_career_pitching.csv')\n",
    "    df_career_pitching.drop(['Unnamed: 0'], axis=1, errors='ignore', inplace=True)\n",
    "    df_career_pitching.drop(['lgID'], axis=1, errors='ignore', inplace=True)\n",
    "else:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Build data set for each pitcher in the pitching database\n",
    "    count = 0\n",
    "    for player in df_pitching['playerID'].unique():\n",
    "        # Set variable to '1' if player is in the HOF, else set to '0'\n",
    "        in_hof = 1 if player in hof_list else 0\n",
    "    \n",
    "        # Get all pitching statistics for this player and sum\n",
    "        ds_pitch = df_pitching.loc[df_batting['playerID'] == player].sum()\n",
    "        \n",
    "        # Get the fielding statistics, only when they were a pitcher, for this player and sum\n",
    "        ds_field = df_fielding.loc[(df_fielding['playerID'] == player) & (df_fielding['POS'] == 'P')].sum()\n",
    "        \n",
    "        # Determine the number of years played (should be the same in both sets, but who knows)\n",
    "        pitch_yrs = ds_pitch['stint']\n",
    "        field_yrs = ds_field['stint']\n",
    "        max_yrs = max(pitch_yrs, field_yrs)\n",
    "        \n",
    "        # Get rid of the fields we don't want in both data sets\n",
    "        ds_pitch.drop(['yearID', 'teamID', 'lgID'], inplace=True, errors='ignore')\n",
    "        ds_field.drop(['playerID', 'yearID', 'stint', 'teamID', 'lgID', 'POS', 'G', 'GS', 'WP'], inplace=True, errors='ignore')\n",
    "        \n",
    "        # Overwrite specific fields\n",
    "        ds_pitch['playerID'] = player\n",
    "        ds_pitch['stint'] = max_yrs\n",
    "        ds_pitch['ERA'] = ds_pitch['ERA'] / pitch_yrs\n",
    "        \n",
    "        # Add 'HOF' field to data set\n",
    "        ds_field['HOF'] = in_hof\n",
    "        \n",
    "        # Append fielding data to batting data and append to the dataframe\n",
    "        df_career_pitching = df_career_pitching.append(ds_pitch.append(ds_field), ignore_index=True)\n",
    "        count += 1\n",
    "    \n",
    "    # The order of the columns gets messed up, so fix at the end\n",
    "    # Move 'stint' to the front, then 'playerID' so we'll get 'playerID' 'stint' ...\n",
    "    # Move 'HOF' to the end of the list so we'll get ... 'HOF'\n",
    "    df_career_pitching.drop(['Unnamed: 0'], axis=1, errors='ignore')\n",
    "    col_name_lst = list(df_career_pitching.columns)\n",
    "    col_name_lst.insert(0, col_name_lst.pop(col_name_lst.index('stint')))\n",
    "    col_name_lst.insert(0, col_name_lst.pop(col_name_lst.index('playerID')))\n",
    "    col_name_lst.append(col_name_lst.pop(col_name_lst.index('HOF')))\n",
    "    df_career_pitching = df_career_pitching[col_name_lst]\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f'It took {elapsed_time} seconds to process the pitching data. count({count})')\n",
    "    \n",
    "    # Save to file so we only need to generate once    \n",
    "    df_career_pitching.to_csv('df_career_pitching.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Fielding Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create new dataframe to hold career stats for fielders\n",
    "# A fielder is anyone showing up in the 'df_fielding' dataframe \n",
    "# AND NOT in the 'df_pitching' dataframe, both were imported above\n",
    "df_career_fielder = pandas.DataFrame()\n",
    "\n",
    "if os.path.isfile('df_career_fielder.csv'):\n",
    "    df_career_fielder = pandas.read_csv('df_career_fielder.csv')\n",
    "else:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Get a list of pitchers, these will be excluded from processing\n",
    "    pitchers = list(df_pitching['playerID'].unique())\n",
    "    \n",
    "    # Build data set for each player in the fielding database\n",
    "    count = 0\n",
    "    for player in df_fielding['playerID'].unique():\n",
    "        \n",
    "        # If the player is a pitcher, exclude them from this dataset\n",
    "        if player in pitchers:\n",
    "            continue\n",
    "        \n",
    "        # Set variable to '1' if player is in the HOF, else set to '0'\n",
    "        in_hof = 1 if player in hof_list else 0\n",
    "        \n",
    "        # Get all batting and fielding information for this player and sum\n",
    "        ds_bat = df_batting.loc[df_batting['playerID'] == player].sum()\n",
    "        ds_fld = df_fielding.loc[df_fielding['playerID'] == player].sum()\n",
    "        \n",
    "        # Determine the number of years played (should be the same in both sets, but who knows)\n",
    "        bat_yrs = ds_bat['stint']\n",
    "        fld_yrs = ds_fld['stint']\n",
    "        max_yrs = max(bat_yrs, fld_yrs)\n",
    "        \n",
    "        # Get rid of the fields we don't want\n",
    "        ds_bat.drop(['yearID', 'teamID', 'lgID', 'POS', 'G'], inplace=True, errors='ignore')\n",
    "        ds_fld.drop(['playerID', 'yearID', 'stint', 'teamID', 'lgID', 'POS', 'G', 'GS', 'SB', 'CS'], inplace=True, errors='ignore')\n",
    "        \n",
    "        # Overwrite specific fields\n",
    "        ds_bat['playerID'] = player\n",
    "        ds_bat['stint'] = max_yrs\n",
    "        \n",
    "        # Add 'HOF' field to data set\n",
    "        ds_fld['HOF'] = in_hof\n",
    "        \n",
    "        # Append fielding data to batting data and append to the dataframe\n",
    "        df_career_fielder = df_career_fielder.append(ds_bat.append(ds_fld), ignore_index=True)\n",
    "        \n",
    "        count += 1\n",
    "    \n",
    "    # The order of the columns gets messed up, so fix at the end\n",
    "    # Move 'stint' to the front, then 'playerID' so we'll get 'playerID' 'stint' ...\n",
    "    # Move 'HOF' to the end of the list so we'll get ... 'HOF'\n",
    "    col_name_lst = list(df_career_fielder.columns)\n",
    "    col_name_lst.insert(0, col_name_lst.pop(col_name_lst.index('stint')))\n",
    "    col_name_lst.insert(0, col_name_lst.pop(col_name_lst.index('playerID')))\n",
    "    col_name_lst.append(col_name_lst.pop(col_name_lst.index('HOF')))\n",
    "    df_career_fielder = df_career_fielder[col_name_lst]\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f'It took {elapsed_time} seconds to process the fielder data. count({count})')\n",
    "\n",
    "    # Save to file so we only need to generate once    \n",
    "    df_career_fielder.to_csv('df_career_fielder.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe to hold season stats for pitchers\n",
    "# A pitcher is anyone showing up in the 'df_pitching' dataframe, which was imported above\n",
    "df_season_pitching = pandas.DataFrame()\n",
    "\n",
    "if os.path.isfile('df_season_pitching.csv'):\n",
    "    df_season_pitching = pandas.read_csv('df_season_pitching.csv')\n",
    "else:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    count = 0\n",
    "\n",
    "    # Process all players in the pitching db\n",
    "    for player in df_pitching['playerID'].unique():\n",
    "        # Process each season for this player \n",
    "        for year in df_pitching.loc[(df_pitching['playerID'] == player)]['yearID'].unique():\n",
    "            # Set variable to '1' if player is in the HOF, else set to '0'\n",
    "            all_star = int(not df_all_star.loc[((df_all_star['playerID'] == player) & \n",
    "                                                (df_all_star['yearID'] == year))].empty)\n",
    "        \n",
    "            # Get all pitching statistics for this player and season\n",
    "            df_pitch = df_pitching.loc[(df_pitching['playerID'] == player) & \n",
    "                                       (df_pitching['yearID'] == year)]\n",
    "\n",
    "            # Get the fielding statistics for this player and season\n",
    "            df_field = df_fielding.loc[(df_fielding['playerID'] == player) & \n",
    "                                       (df_fielding['yearID'] == year) &\n",
    "                                       (df_fielding['POS'] == 'P')]\n",
    "            \n",
    "            # There may be more than one entry for a player + year (changed teams?)\n",
    "            # Average all stats for each column.\n",
    "            ds_pitch = df_pitch.mean()\n",
    "            ds_field = df_field.mean()\n",
    "        \n",
    "            # Get rid of the fields we don't want in both data sets\n",
    "            ds_pitch.drop(['stint', 'teamID', 'lgID'], inplace=True, errors='ignore')\n",
    "            ds_field.drop(['playerID', 'yearID', 'stint', 'teamID', 'lgID', 'POS', 'G', 'GS', 'WP'], inplace=True, errors='ignore')\n",
    "\n",
    "            # Add 'playerID' to pitch data set (it was lost during 'mean' step \n",
    "            # Add all-star' field to data set\n",
    "            ds_pitch['playerID'] = player\n",
    "            ds_field['all-star'] = all_star\n",
    "            \n",
    "            # Append fielding data to batting data and append to the dataframe\n",
    "            df_season_pitching = df_season_pitching.append(ds_pitch.append(ds_field), ignore_index=True)\n",
    "            \n",
    "            count += 1\n",
    "    \n",
    "    # The order of the columns gets messed up, so fix at the end\n",
    "    # Move 'yearID' to the front, then 'playerID' so we'll get 'playerID' 'yearID' ...\n",
    "    # Move 'all-star' to the end of the list so we'll get ... 'all-star'\n",
    "    #df_career_pitching.drop(['Unnamed: 0'], axis=1, errors='ignore')\n",
    "    col_name_lst = list(df_season_pitching.columns)\n",
    "    col_name_lst.insert(0, col_name_lst.pop(col_name_lst.index('yearID')))\n",
    "    col_name_lst.insert(0, col_name_lst.pop(col_name_lst.index('playerID')))\n",
    "    col_name_lst.append(col_name_lst.pop(col_name_lst.index('all-star')))\n",
    "    df_season_pitching = df_season_pitching[col_name_lst]\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f'It took {elapsed_time} seconds to process the pitching data. count({count})')\n",
    "    \n",
    "    # Save to file so we only need to generate once    \n",
    "    df_season_pitching.to_csv('df_season_pitching.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe to hold season stats for pitchers\n",
    "# A pitcher is anyone showing up in the 'df_pitching' dataframe, which was imported above\n",
    "df_season_fielding = pandas.DataFrame()\n",
    "\n",
    "if os.path.isfile('df_season_fielding.csv'):\n",
    "    df_season_fielding = pandas.read_csv('df_season_fielding.csv')\n",
    "else:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    count = 0\n",
    "\n",
    "    # Get a list of pitchers, these will be excluded from processing\n",
    "    pitchers = list(df_pitching['playerID'].unique())\n",
    "    \n",
    "    # Process all players in the fielding db\n",
    "    for player in df_fielding['playerID'].unique():\n",
    "        \n",
    "        # If the player is a pitcher, exclude them from this dataset\n",
    "        if player in pitchers:\n",
    "            continue\n",
    "        \n",
    "        # Process each season for this player \n",
    "        for year in df_fielding.loc[(df_fielding['playerID'] == player)]['yearID'].unique():\n",
    "            # Set variable to '1' if player is in the HOF, else set to '0'\n",
    "            all_star = int(not df_all_star.loc[((df_all_star['playerID'] == player) & \n",
    "                                                (df_all_star['yearID'] == year))].empty)\n",
    "        \n",
    "            # Get all batting statistics for this player and season\n",
    "            df_hit = df_batting.loc[(df_batting['playerID'] == player) & \n",
    "                                    (df_batting['yearID'] == year)]\n",
    "\n",
    "            # Get the fielding statistics for this player and season\n",
    "            df_field = df_fielding.loc[(df_fielding['playerID'] == player) & \n",
    "                                       (df_fielding['yearID'] == year)]\n",
    "            \n",
    "            # There may be more than one entry for a player + year (changed teams?)\n",
    "            # Average all stats for each column.\n",
    "            ds_hit = df_hit.mean()\n",
    "            ds_field = df_field.mean()\n",
    "        \n",
    "            # Get rid of the fields we don't want in both data sets\n",
    "            ds_hit.drop(['stint'], inplace=True, errors='ignore')\n",
    "            ds_field.drop(['yearID', 'stint', 'G', 'CS', 'SB'], inplace=True, errors='ignore')\n",
    "\n",
    "            # Add 'playerID' to pitch data set (it was lost during 'mean' step \n",
    "            # Add all-star' field to data set\n",
    "            ds_hit['playerID'] = player\n",
    "            ds_field['all-star'] = all_star\n",
    "            \n",
    "            # Append fielding data to batting data and append to the dataframe\n",
    "            df_season_fielding = df_season_fielding.append(ds_hit.append(ds_field), ignore_index=True)\n",
    "            \n",
    "            count += 1\n",
    "    \n",
    "    # The order of the columns gets messed up, so fix at the end\n",
    "    # Move 'yearID' to the front, then 'playerID' so we'll get 'playerID' 'yearID' ...\n",
    "    # Move 'all-star' to the end of the list so we'll get ... 'all-star'\n",
    "    col_name_lst = list(df_season_fielding.columns)\n",
    "    col_name_lst.insert(0, col_name_lst.pop(col_name_lst.index('yearID')))\n",
    "    col_name_lst.insert(0, col_name_lst.pop(col_name_lst.index('playerID')))\n",
    "    col_name_lst.append(col_name_lst.pop(col_name_lst.index('all-star')))\n",
    "    df_season_fielding = df_season_fielding[col_name_lst]\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f'It took {elapsed_time} seconds to process the fielding data. count({count})')\n",
    "    \n",
    "    # Save to file so we only need to generate once    \n",
    "    df_season_fielding.to_csv('df_season_fielding.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of career statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------Summary--------------------------\n",
      "Total number of pitchers - 9655\n",
      "Total number of fielders - 9574\n",
      "Total number of players - 19229\n",
      "--------------------------------------------------\n",
      "Number of pitchers in HOF (96) or  (0.994)%\n",
      "Number of fielders in HOF (134) or  (1.400)%\n",
      "Total in HOF (256) or (1.331)%\n"
     ]
    }
   ],
   "source": [
    "num_fielders = df_career_fielder.count()[0]\n",
    "num_pitchers = df_career_pitching.count()[0]\n",
    "num_players = num_fielders + num_pitchers\n",
    "num_hof = df_hof.count()\n",
    "pct_hof = (num_hof / num_players) * 100\n",
    "\n",
    "hofers = set(df_hof.unique())\n",
    "pitchers = set(df_career_pitching['playerID'].unique())\n",
    "fielders = set(df_career_fielder['playerID'].unique())\n",
    "num_hof_pitchers = len(pitchers.intersection(hofers))\n",
    "pct_hof_pitchers = (num_hof_pitchers / num_pitchers) * 100\n",
    "num_hof_fielders = len(fielders.intersection(hofers))\n",
    "pct_hof_fielders = (num_hof_fielders / num_fielders) * 100\n",
    "\n",
    "print('-----------------Summary--------------------------')\n",
    "print(f'Total number of pitchers - {num_pitchers}')\n",
    "print(f'Total number of fielders - {num_fielders}')\n",
    "print(f'Total number of players - {num_players}')\n",
    "print('--------------------------------------------------')\n",
    "print(f'Number of pitchers in HOF ({num_hof_pitchers}) or  ({pct_hof_pitchers:0.3f})%')\n",
    "print(f'Number of fielders in HOF ({num_hof_fielders}) or  ({pct_hof_fielders:0.3f})%')\n",
    "print(f'Total in HOF ({num_hof}) or ({pct_hof:0.3f})%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These statistics show how hard it is for a player to make the hall of fame.  This also means that the data set is exteremely unbalanced where there are more players who do NOT make the HOF than do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of single season statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------Summary--------------------------\n",
      "Total number of seasons played   - 49390\n",
      "Total number of all-star seasons - 5291\n",
      "Percent all-star seeasons        - 10.713\n"
     ]
    }
   ],
   "source": [
    "num_seasons_played = df_season_fielding.count()[0]\n",
    "num_seasons_all_star = df_allstar.count()\n",
    "pct_seasons_all_star = (num_seasons_all_star / num_seasons_played) * 100\n",
    "print('-----------------Summary--------------------------')\n",
    "print(f'Total number of seasons played   - {num_seasons_played}')\n",
    "print(f'Total number of all-star seasons - {num_seasons_all_star}')\n",
    "print(f'Percent all-star seeasons        - {pct_seasons_all_star:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xdata type(<class 'pandas.core.frame.DataFrame'>) size((9655, 35))\n"
     ]
    }
   ],
   "source": [
    "# Slice dataframe.  Get all data except 'playerId' and 'HOF'\n",
    "# NOTE: use 'fillna(0)' to replace 'nan' with 0\n",
    "Xdata = df_career_pitching.loc[:,'stint':'ZR'].fillna(0)\n",
    "print(f'Xdata type({type(Xdata)}) size({Xdata.shape})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain type(<class 'numpy.ndarray'>) size((9655, 36))\n"
     ]
    }
   ],
   "source": [
    "# Convert pandas dataframe to numpy array\n",
    "Xtrain = Xdata.reset_index().values.astype(int)\n",
    "print(f'Xtrain type({type(Xtrain)}) size({Xtrain.shape})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tdata type(<class 'pandas.core.series.Series'>) size((9655,))\n"
     ]
    }
   ],
   "source": [
    "# Slice dataframe.  Get only 'HOF'\n",
    "Tdata = df_career_pitching.loc[:,'HOF']\n",
    "print(f'Tdata type({type(Tdata)}) size({Tdata.shape})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttrain type(<class 'numpy.ndarray'>) size((9655,))\n"
     ]
    }
   ],
   "source": [
    "Ttrain = Tdata.reset_index().values[:,1].astype(int)\n",
    "print(f'Ttrain type({type(Ttrain)}) size({Ttrain.shape})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will be our 'classes'\n",
    "# 0 = Not a HOF player, 1 = HOF player\n",
    "np.unique(Ttrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet = NeuralNetwork_Convolutional(n_channels_in_image=1, #Xtrain.shape[0],\n",
    "                                   image_size=Xtrain.shape[0],\n",
    "                                   n_units_in_conv_layers=[5],\n",
    "                                   n_units_in_fc_hidden_layers=[2],\n",
    "                                   classes=np.unique(Ttrain),\n",
    "                                   kernels_size_and_stride=[[5, 2]],\n",
    "                                   use_gpu=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight 5 1, but got 2-dimensional input of size [9655, 36] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-bb5852292e78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-6fa9f51d3ee5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, T, n_epochs, learning_rate)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_F\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/mnt/workspace/bstaab/CS/cs545/venv/lib64/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/mnt/workspace/bstaab/CS/cs545/venv/lib64/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/mnt/workspace/bstaab/CS/cs545/venv/lib64/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/mnt/workspace/bstaab/CS/cs545/venv/lib64/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/mnt/workspace/bstaab/CS/cs545/venv/lib64/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    338\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    339\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 340\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight 5 1, but got 2-dimensional input of size [9655, 36] instead"
     ]
    }
   ],
   "source": [
    "nnet.train(Xtrain, Ttrain, 50, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
